<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[openshift部署之前环境准备]]></title>
    <url>%2F2017%2F07%2F17%2Fopenstack%2Fopenshift%20%E9%83%A8%E7%BD%B2%E4%B9%8B%E5%89%8D%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87%2F</url>
    <content type="text"><![CDATA[openshift部署之前环境准备准备工作(Prerequisites)Install stand-alone registry To install OpenShift Origin as a stand-alone registry, see Installing a Stand-alone Registry. System Requirements Master 1. base os: Fedora 21, CentOS 7.3, or RHEL 7.3 with the &quot;Minimal&quot; installation option and the latest packages from the Extras channel, or RHEL Atomic Host 7.3.2 or later. RHEL 7.2 is also supported using Docker 1.12 and its dependencies. 2. 2vcpu 3. Minimum 16 GB RAM 4. Minimum 40 GB hard disk space for the file system containing /var/. Nodes 1. Physical or virtual system, or an instance running on a public or private IaaS. 2. Base OS: Fedora 21, CentOS 7.3, or RHEL 7.3 or later with &quot;Minimal&quot; installation option, or RHEL Atomic Host 7.3.2 or later. RHEL 7.2 is also supported using Docker 1.12 and its dependencies. 3. NetworkManager 1.0 or later. 4. 1 vCPU. 5. Minimum 8 GB RAM. 6. Minimum 15 GB hard disk space for the file system containing /var/. 7. An additional minimum 15 GB unallocated space to be used for Docker’s storage back end; see Configuring Docker Storage. External etcd Nodes 1. Minimum 20 GB hard disk space for etcd data. 2. Consult Hardware Recommendations to properly size your etcd nodes. 3. Currently, OpenShift Origin stores image, build, and deployment metadata in etcd. You must periodically prune old resources. If you are planning to leverage a large number of images/builds/deployments, place etcd on machines with large amounts of memory and fast SSD drives. openshift设置缓存openshift master 主机节点高速缓存目的是为了缓解CPU负载。然而，在小于1000pod的较小集群中，该缓存可能会浪费大量内存，从而忽略CPU负载的降低。 默认缓存大小为50000个条目，根据资源大小，可以增加到1到2Gb的内存。可以使用／etc/origin/master/master-config.yaml中的以下设置来减少此缓存大小。 1234kubernetesMasterConfig: apiServerArguments: deserialization-cache-size: - &quot;1000&quot; 设置core使用默认情况下，openshift origin master和node节点使用他们运行系统中的所有内核，你可以通过设置GOMAXPROCS环境变量来选择 我们所希望openshfit使用的内核数；12# openshift使用1个内核export GOMAXPROCS=1 SELinuxSecurity-Enhanced Linux (SELinux)在安装openshit之前必须在所以节点打开（这点好怪，其他的例如openstack要关闭）不然会安装失败。在 /etc/selinux/config 文件中配置SELINUXTYPE=targeted 1234567891011# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.SELINUX=enforcing# SELINUXTYPE= can take one of these three values:# targeted - Targeted processes are protected,# minimum - Modification of targeted policy. Only selected processes are protected.# mls - Multi Level Security protection.SELINUXTYPE=targeted NTP时间同步不用多说，设置openshift_clock_enabled to true打开1openshift_clock_enabled to true DNSOpenShift Origin在环境中需要一个功能齐全的DNS服务器。 这是理想的运行DNS软件的独立主机，可以为在平台上运行的主机和容器提供名称解析。 默认情况下，容器从主机收到dns配置文件进行解析(/etc/reslov.conf) contaier dns openshift origin 将dns值插入到pods（节点的命名空间里面），这个值在/etc/orgin/node/node-config.yaml文件中由dnsIP参数定义，dnsIP参数默认设置为主机节点的地址，因为主机使用dnsmasq. 如果在node-config.yaml文件中忽略dnpIP这个参数，就会默认为kubernetes service ip， 它是该pod的/etc/resolv.conf文件中的第一个名称服务器。 从OpenShift origin 1.2开始， dnsmasq自动配置到所以主节点（master）和节点（node）上。pod使用节点作为它的dns，然后节点接受所有转发请求。默认情况下，dnsmasq在节点上进行配置监听 53端口， 所以节点不能运行其他的类型的dns应用程序。 节点上需要NetworkManager才能使用DNS IP地址填充dnsmasq. 下面是一个例子 123master A 10.64.33.100node1 A 10.64.33.101node2 A 10.64.33.102 如果你没有正常运行dns缓解，你下面的操作可能会失败： 通过ansible脚本去安装产品 基础设施容器的部署（registry， routers） 访问OpenShif Origin Web 控制台， 因为它不能通过ip地址单独访问。 配置hosts使用dns 确保您的环境中的每个主机都配置为从DNS服务器解析主机名。主机DNS解析的配置取决于是否启用DHCP。 如果dhcp服务器是禁用的， 应该将您的网络接口配置为静态，并将DNS名称服务器添加到NetworkManager。 如果dhcp服务器是启用的，NetworkManager调度脚本将根据DHCP配置自动配置DNS。或者，您可以在node-config.yaml文件中的dnsIP中添加一个值，以将pod的resolv.conf文件添加到前面。然后，第二个名称服务器由主机的第一个名称服务器定义。默认情况下，这将是节点主机的IP地址。 对于大多数配置，请勿在OpenShift Origin（使用Ansible）高级安装期间设置openshift_dns_ip选项，因为此选项将覆盖由dnsIP设置的默认IP地址。 相反，允许安装程序配置每个节点使用dnsmasq并将请求转发给SkyDNS或外部DNS提供程序。如果您设置了openshift_dns_ip选项，则应该使用首先查询SkyDNS的DNS IP或SkyDNS服务或端点IP（Kubernetes服务IP）进行设置。 验证dns 服务器能够解析hosts 检查/etc/resolv.conf内容 12345$ cat /etc/resolv.conf# Generated by NetworkManagersearch example.comnameserver 10.64.33.1# nameserver updated by /etc/NetworkManager/dispatcher.d/99-origin-dns.sh 在这个例子中， 10.64.33.1 就是我们的DNS server. 2.测试/etc/resolv.conf中列出的DNS服务器是否可以将主机名解析为OpenShift Origin环境中所有主节点和IP节点的IP地址 123456# dig &lt;node_hostname&gt; @&lt;IP_address&gt; +short$ dig master.example.com @10.64.33.1 +short10.64.33.100$ dig node1.example.com @10.64.33.1 +short10.64.33.101]]></content>
      <categories>
        <category>Openshift</category>
      </categories>
      <tags>
        <tag>openshift</tag>
        <tag>pass</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kolla之reconfigure]]></title>
    <url>%2F2017%2F07%2F14%2Fopenstack%2Fkolla-reconfigure%2F</url>
    <content type="text"><![CDATA[今天好好理理kolla reconfigure如何玩的 kolla的配置管理主要是管理openstack service config文件；主要实现是 1kolla-ansible reconfigure reconfigure 使用下面以1个例子来演示下 修改nova.conf,重启相关服务 在部署节点 新建／etc/kolla/config/nova/nova.conf 123456### 修改rpc_response超时时间vim ／etc/kolla/config/nova/nova.confrpc_response_timeout = 350## 执行reconfigurekolla-ansible reconfigure reconfigure 代码流程kolla-ansible 的核心代码在ansible实现的先介绍下ansible role是什么？如何使用先看看下面nova role的代码目录 12345678910111213141516[root@control01 ansible]# tree -d.├── action_plugins├── group_vars├── inventory├── library└── roles ├── nova │ ├── defaults │ ├── handlers │ ├── meta │ ├── tasks │ └── templates . . . ansible role是什么？Ansible Role 是一种分类 &amp; 重用的概念，透过将 vars, tasks, files, templates, handler … 等等根据不同的目的(例如：nova、glance、cinder)，规划后至于独立目录中，后续便可以利用 include 的概念來使用。 若同样是 include 的概念，那 role 跟 include 之间不一样的地方又是在哪里呢? 答案是：role 的 include 机制是自动的! 我們只要事前將 role 的 vars / tasks / files / handler …. 等等事先定义好按照特定的结构(下面會提到)放好，Ansible 就會很聰明的幫我們 include 完成，不需要再自己一个一个指定 include。 透过这样的方式，管理者可以透过設定 role 的方式將所需要安裝設定的功能分门别类，拆成细项來管理并编写相对应的 script，让原本可能很庞大的设定工作可以细分成多个不同的部分來分別设定，不仅仅可以让自己重复利用特定的设定，也可以共享給其他人一同使用。 要设计一個 role，必須先知道怎麼將 files / templates / tasks / handlers / vars …. 等等拆开设定 看看下面kolla-ansible下nova role的代码目录 12345678910111213141516[root@control01 ansible]# tree -d.├── action_plugins├── group_vars├── inventory├── library└── roles ├── nova │ ├── defaults │ ├── handlers │ ├── meta │ ├── tasks │ └── templates . . . 以上就是一个基本完整的一个role结构，当然还有file、vars；我们这里没有使用这个2个；如果没有的部分可以不用。 ansible会针对role（x）进行以下处理： 如果role/x／tasks／main.yml存在，则会自动加到playbook中的task list中 如果role/x／handlers／main.yml存在，则会自动加到playbook中的handler list中 如果role/x/vars/mani.yml存在，则会自动加入到playbook中的variables list中 如果role/x/meta/main.yml存在, 任何与指定的role想依赖的其他的role设置都会被自动加入 roles/x/templates/ 目录中的 template tasks，在 playbook 中使用时不需要指定绝对(absolutely) or 相对(relatively)路径 在 roles/x/files/ 目录中的 copy tasks 或是 script tasks，在 playbook 中使用时不需要指定绝对(absolutely) or 相对(relatively)路径 定义在 roles/x/defaults/main.yml 中的变量將会是使用该role时所取得的预设变量 再返回来看来 kolla-ansible reconfigure 具体代码就能理解了 12[root@control01 ansible]# kolla-ansible reconfigureReconfigure OpenStack service : ansible-playbook -i /usr/share/kolla-ansible/ansible/inventory/all-in-one -e @/etc/kolla/globals.yml -e @/etc/kolla/passwords.yml -e CONFIG_DIR=/etc/kolla -e action=reconfigure -e serial=0 /usr/share/kolla-ansible/ansible/site.yml 1234## glance tasks main.yaml[root@control01 tasks]# cat /root/kolla-ansible/ansible/roles/glance/tasks/main.yml---- include: &quot;&#123;&#123; action &#125;&#125;.yml&quot; 从上面命令就可以看出，tasks是根据action去找相应的操作。那我们去跳到reconfigure.yml 123[root@control01 tasks]# cat reconfigure.yml---- include: deploy.yml 12345678910111213141516171819202122232425262728293031[root@control01 tasks]# cat deploy.yml---- include: ceph.yml when: - (enable_ceph | bool) and (glance_backend_ceph | bool) - inventory_hostname in groups[&apos;ceph-mon&apos;] or inventory_hostname in groups[&apos;glance-api&apos;] or inventory_hostname in groups[&apos;glance-registry&apos;]- include: external_ceph.yml when: - (enable_ceph | bool == False) and (glance_backend_ceph | bool) - inventory_hostname in groups[&apos;glance-api&apos;] or inventory_hostname in groups[&apos;glance-registry&apos;]- include: register.yml when: inventory_hostname in groups[&apos;glance-api&apos;]- include: config.yml when: inventory_hostname in groups[&apos;glance-api&apos;] or inventory_hostname in groups[&apos;glance-registry&apos;]- include: bootstrap.yml when: inventory_hostname in groups[&apos;glance-api&apos;]- name: Flush handlers meta: flush_handlers- include: check.yml when: inventory_hostname in groups[&apos;glance-api&apos;] or inventory_hostname in groups[&apos;glance-registry&apos;] 整体的一个脉络流程就是这样；下面我们一个一个去细看他们的实现。 这里我们主要是看config.yml和flush_handlers这个2个的实现 config.ymlconfig.yml 是用来生成openstack sevice config文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182[root@control01 tasks]# cat config.yml---### 生成 node_config_directory（默认是/etc/kolla， 你可以在all.yml中设置）sevice目录- name: Ensuring config directories exist file: path: &quot;&#123;&#123; node_config_directory &#125;&#125;/&#123;&#123; item.key &#125;&#125;&quot; state: &quot;directory&quot; recurse: yes when: - inventory_hostname in groups[item.value.group] - item.value.enabled | bool with_dict: &quot;&#123;&#123; glance_services &#125;&#125;&quot;### copy config.json file 到node_config_directory/service 目录下- name: Copying over config.json files for services template: src: &quot;&#123;&#123; item.key &#125;&#125;.json.j2&quot; dest: &quot;&#123;&#123; node_config_directory &#125;&#125;/&#123;&#123; item.key &#125;&#125;/config.json&quot; register: glance_config_jsons when: - item.value.enabled | bool - inventory_hostname in groups[item.value.group] with_dict: &quot;&#123;&#123; glance_services &#125;&#125;&quot; notify: - Restart glance-api container - Restart glance-registry container- name: Copying over glance-*.conf merge_configs: vars: service_name: &quot;&#123;&#123; item.key &#125;&#125;&quot; sources: - &quot;&#123;&#123; role_path &#125;&#125;/templates/&#123;&#123; item.key &#125;&#125;.conf.j2&quot; - &quot;&#123;&#123; node_custom_config &#125;&#125;/global.conf&quot; - &quot;&#123;&#123; node_custom_config &#125;&#125;/database.conf&quot; - &quot;&#123;&#123; node_custom_config &#125;&#125;/messaging.conf&quot; - &quot;&#123;&#123; node_custom_config &#125;&#125;/glance.conf&quot; - &quot;&#123;&#123; node_custom_config &#125;&#125;/glance/&#123;&#123; item.key &#125;&#125;.conf&quot; - &quot;&#123;&#123; node_custom_config &#125;&#125;/glance/&#123;&#123; inventory_hostname &#125;&#125;/&#123;&#123; item.key &#125;&#125;.conf&quot; dest: &quot;&#123;&#123; node_config_directory &#125;&#125;/&#123;&#123; item.key &#125;&#125;/&#123;&#123; item.key &#125;&#125;.conf&quot; register: glance_confs when: - item.value.enabled | bool - inventory_hostname in groups[item.value.group] with_dict: &quot;&#123;&#123; glance_services &#125;&#125;&quot; notify: - Restart glance-api container - Restart glance-registry container- name: Check if policies shall be overwritten local_action: stat path=&quot;&#123;&#123; node_custom_config &#125;&#125;/glance/policy.json&quot; register: glance_policy- name: Copying over existing policy.json template: src: &quot;&#123;&#123; node_custom_config &#125;&#125;/glance/policy.json&quot; dest: &quot;&#123;&#123; node_config_directory &#125;&#125;/&#123;&#123; item.key &#125;&#125;/policy.json&quot; register: glance_policy_jsons when: - glance_policy.stat.exists - inventory_hostname in groups[item.value.group] with_dict: &quot;&#123;&#123; glance_services &#125;&#125;&quot; notify: - Restart glance-api container - Restart glance-registry container- name: Check glance containers kolla_docker: action: &quot;compare_container&quot; common_options: &quot;&#123;&#123; docker_common_options &#125;&#125;&quot; name: &quot;&#123;&#123; item.value.container_name &#125;&#125;&quot; image: &quot;&#123;&#123; item.value.image &#125;&#125;&quot; volumes: &quot;&#123;&#123; item.value.volumes &#125;&#125;&quot; register: check_glance_containers when: - action != &quot;config&quot; - inventory_hostname in groups[item.value.group] - item.value.enabled | bool with_dict: &quot;&#123;&#123; glance_services &#125;&#125;&quot; notify: - Restart glance-api container - Restart glance-registry container 这里最核心的实现就是merge_configs。action plugin中在merge_configs.py作用是导入template模板，并且run。代码在/usr/share/kolla-ansible/ansible/action_plugins/merge_configs.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138import collectionsimport inspectimport osfrom ansible.plugins import actionfrom six import StringIOfrom oslo_config import iniparserclass OverrideConfigParser(iniparser.BaseParser): def __init__(self): self._cur_sections = collections.OrderedDict() self._sections = collections.OrderedDict() self._cur_section = None def assignment(self, key, value): cur_value = self._cur_section.get(key) if len(value) == 1 and value[0] == &apos;&apos;: value = [] if not cur_value: self._cur_section[key] = [value] else: self._cur_section[key].append(value) def parse(self, lineiter): self._cur_sections = collections.OrderedDict() super(OverrideConfigParser, self).parse(lineiter) # merge _cur_sections into _sections for section, values in self._cur_sections.items(): if section not in self._sections: self._sections[section] = collections.OrderedDict() for key, value in values.items(): self._sections[section][key] = value def new_section(self, section): cur_section = self._cur_sections.get(section) if not cur_section: cur_section = collections.OrderedDict() self._cur_sections[section] = cur_section self._cur_section = cur_section return cur_section def write(self, fp): def write_key_value(key, values): for v in values: if not v: fp.write(&apos;&#123;&#125; =\n&apos;.format(key)) for index, value in enumerate(v): if index == 0: fp.write(&apos;&#123;&#125; = &#123;&#125;\n&apos;.format(key, value)) else: fp.write(&apos;&#123;&#125; &#123;&#125;\n&apos;.format(len(key)*&apos; &apos;, value)) def write_section(section): for key, values in section.items(): write_key_value(key, values) for section in self._sections: fp.write(&apos;[&#123;&#125;]\n&apos;.format(section)) write_section(self._sections[section]) fp.write(&apos;\n&apos;)class ActionModule(action.ActionBase): TRANSFERS_FILES = True def read_config(self, source, config): # Only use config if present if os.access(source, os.R_OK): with open(source, &apos;r&apos;) as f: template_data = f.read() result = self._templar.template(template_data) fakefile = StringIO(result) config.parse(fakefile) fakefile.close() def run(self, tmp=None, task_vars=None): if task_vars is None: task_vars = dict() result = super(ActionModule, self).run(tmp, task_vars) # NOTE(jeffrey4l): Ansible 2.1 add a remote_user param to the # _make_tmp_path function. inspect the number of the args here. In # this way, ansible 2.0 and ansible 2.1 are both supported make_tmp_path_args = inspect.getargspec(self._make_tmp_path)[0] if not tmp and len(make_tmp_path_args) == 1: tmp = self._make_tmp_path() if not tmp and len(make_tmp_path_args) == 2: remote_user = (task_vars.get(&apos;ansible_user&apos;) or self._play_context.remote_user) tmp = self._make_tmp_path(remote_user) sources = self._task.args.get(&apos;sources&apos;, None) extra_vars = self._task.args.get(&apos;vars&apos;, list()) if not isinstance(sources, list): sources = [sources] temp_vars = task_vars.copy() temp_vars.update(extra_vars) config = OverrideConfigParser() old_vars = self._templar._available_variables self._templar.set_available_variables(temp_vars) for source in sources: self.read_config(source, config) self._templar.set_available_variables(old_vars) # Dump configparser to string via an emulated file fakefile = StringIO() config.write(fakefile) remote_path = self._connection._shell.join_path(tmp, &apos;src&apos;) xfered = self._transfer_data(remote_path, fakefile.getvalue()) fakefile.close() new_module_args = self._task.args.copy() new_module_args.pop(&apos;vars&apos;, None) new_module_args.pop(&apos;sources&apos;, None) new_module_args.update( dict( src=xfered ) ) result.update(self._execute_module(module_name=&apos;copy&apos;, module_args=new_module_args, task_vars=task_vars, tmp=tmp)) return result]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>kolla</tag>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[介绍 kolla]]></title>
    <url>%2F2017%2F07%2F14%2Fopenstack%2Fkolla%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[kolla总结在kolla项目下也快1年啦，也该总结下, 文笔不好权当记录。 kolla的定位实现生产级别容器化openstack平台，做到开箱即用 kolla的实现kolla实现主要以docker和ansible实现的。ansible实现编排和远程推送docker实现openstack服务容器化 kolla的实现带来什么变化1.让安装变得更加简单 2.环境一致性 3.让openstack升级不是困难 4.灵活性大大加强 5.openstack服务微服务化 kolla代码目录 从顶层文件目录结构来看，各个目录所包含的内容为： ansible ansible 配置目录里面包含了openstack service ansible role的实现， Kolla - Kolla with ansible!，是kolla-ansible最为重要的一部分。 contrib 示例目录。里面包含了 heat 的编排配置和magnum的一些例子 ； docs 文档目录。也是非常重要的目录，里面包括开发环境设置、镜像编译、Kolla 环境变量等说明，建议把文档都认真读一遍 ； LICENSE LICENSE文件。Apache License Version 2 的 License 文件，没什么好说的。 README.md 说明文件。 specs spec目录。目前只有一个 spec，说明使用容器安装 OpenStack 的理念和优势。 test-requirements.txt python 的 requirements 文件。用于说明测试时所需要的 python 包，目前只有一个 PyYAML。 tests 测试目录。这个目录应该包含 Kolla 的测试套件，但目前只有一个 setup_docker.sh 用于安装 docker。 tools 工具脚本目录。目录包含编译 docker 镜像、清理 docker 环境、生成 Kolla 环境变量、Kolla 启动脚本、json/yaml文件检验等脚本。建议把这个目录的脚本都看一遍，需要点 Shell、python的知识。 tox.ini tox配置文件。tox是一个标准自动化测试工具，python里的。目前这个文件很简单，只包含了 virtualenv 设置和一些简单的检查。 现在 Kolla 还小，不像 nova / neutron 这些庞然大物，因此很值得把 Kolla 的代码认真看看，整理 Kolla 的设计思路，对以后把握 Kolla 的发展很有帮助，也希望大家能参与到 Kolla 的社区中，无论是贡献代码、写写 Blog、找茬，都能帮助到 Kolla 关于kolla方面的文章沙克写的很好，大家可以看看。这个他的博客链接：http://www.chenshake.com/cloud-computing/]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>kolla</tag>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rally-first]]></title>
    <url>%2F2017%2F07%2F06%2Fopenstack%2Frally%2F</url>
    <content type="text"><![CDATA[openstack 测试介绍Rally 介绍介绍贴图：文字介绍 使用kolla部署rallyrally的功能介绍rally的基本使用rally调用tempestrally的局限性OpenStack，毫无疑问是各种服务联合的一个庞大的生态系统。 Rally 作为一个标准测试工具，回答了 “OpenStack 如何大规模运作？” 的问题。 为了使其成为可能，Rally 自动化并统一了多个节点的 OpenStack 部署、云的验证、标准测试和分析。Rally 使用一个通用的方法，使得我们能够检查 OpenStack 是否正常运作，也即，在高负荷下 1K 服务器的安装是否成功。因此，它可以作为 OpenStack CI/CD 系统的基本工具来使用，可以持续改进其 SLA、运作和稳定性。 Rally 的操作如下图所示：http://coffeechou.github.io/public/imgs/rally_actions.png 在kolla中使用rally12345678910#在配置文件打开rallyvim /etc/kolla/globals.ymlenable_rally: &quot;yes&quot;#部署环境kolla-ansible deploy#生成openrc环境变量kolla-ansible post-deploy 初始化rallyRegistering an OpenStack deployment in Rally123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127# 进入rally 容器[root@control01 ~]# docker exec -it -u root rally bash(rally)[root@control01 /]#(rally)[root@control01 deployments]# vim existing-keystone-v3.json&#123; &quot;type&quot;: &quot;ExistingCloud&quot;, &quot;auth_url&quot;: &quot;http://172.16.130.210:5000/v3&quot;, &quot;region_name&quot;: &quot;RegionOne&quot;, &quot;endpoint_type&quot;: &quot;public&quot;, &quot;admin&quot;: &#123; &quot;username&quot;: &quot;admin&quot;, &quot;password&quot;: &quot;99cloud&quot;, &quot;user_domain_name&quot;: &quot;deault&quot;, &quot;project_name&quot;: &quot;admin&quot;, &quot;project_domain_name&quot;: &quot;deault&quot; &#125;, &quot;https_insecure&quot;: false, &quot;https_cacert&quot;: &quot;&quot;&#125;### 创建deployments(rally)[root@control01 deployments]#rally deployment create --file existing-keystone-v3.json --name 99cloud-regionOne+--------------------------------------+---------------------+-------------------+------------------+--------+| uuid | created_at | name | status | active |+--------------------------------------+---------------------+-------------------+------------------+--------+| e36d802c-2211-4bac-a8ea-2f3b967568b9 | 2017-05-08 03:07:46 | 99cloud-regionOne | deploy-&gt;finished | |+--------------------------------------+---------------------+-------------------+------------------+--------+Using deployment: e36d802c-2211-4bac-a8ea-2f3b967568b9~/.rally/openrc was updatedHINTS:* To use standard OpenStack clients, set up your env by running: source ~/.rally/openrc OpenStack clients are now configured, e.g run: openstack image list(rally)[root@control01 deployments]# rally deployment list+--------------------------------------+---------------------+-------------------+-------------------+--------+| uuid | created_at | name | status | active |+--------------------------------------+---------------------+-------------------+-------------------+--------+| af993ae9-363e-4701-b343-626063eaf715 | 2017-04-26 07:13:50 | existing | cleanup-&gt;finished | || e36d802c-2211-4bac-a8ea-2f3b967568b9 | 2017-05-08 03:07:46 | 99cloud-regionOne | deploy-&gt;finished | * |+--------------------------------------+---------------------+-------------------+-------------------+--### 还有一种创建方法，就是使用环境变量去创建### 我们先删除之前创建的(rally)[root@control01 deployments]# rally deployment destroy 99cloud-regionOne(rally)[root@control01 deployments]# rally deployment list+--------------------------------------+---------------------+----------+-------------------+--------+| uuid | created_at | name | status | active |+--------------------------------------+---------------------+----------+-------------------+--------+| af993ae9-363e-4701-b343-626063eaf715 | 2017-04-26 07:13:50 | existing | cleanup-&gt;finished | |+--------------------------------------+---------------------+----------+-------------------+--------+### (rally)[root@control01 /]# cat openrcexport OS_REGION_NAME=RegionOneexport OS_PROJECT_DOMAIN_NAME=defaultexport OS_USER_DOMAIN_NAME=defaultexport OS_PROJECT_NAME=adminexport OS_TENANT_NAME=adminexport OS_USERNAME=adminexport OS_PASSWORD=99cloudexport OS_AUTH_URL=http://172.16.130.210:35357/v3export OS_INTERFACE=internalexport OS_IDENTITY_API_VERSION=3(rally)[root@control01 /]# source openrc## 测试下环境变量是否可用(rally)[root@control01 deployments]# openstack image list+--------------------------------------+-----------------------------------+--------+| ID | Name | Status |+--------------------------------------+-----------------------------------+--------+| 83dc79c8-c6df-4e23-808c-0209778be1b2 | 123 | active || 9d4cc662-bdfb-46cb-996d-3b773ee161d5 | a1 | active || 255cc63c-f635-4bbb-850c-e319d2e4c17d | cenos-raw | active || 63c5d475-e96a-4c1f-985b-12685aa2a95b | centos6.5 | active || 15ed4618-4b3b-4cc3-a9ef-6967e3a3091d | centos7 | active || 15f04241-38cc-482d-b6a3-42ba9d29f47a | cirros | active || 6491747b-2fea-4d7c-b034-b3fe5089df2c | d1 | active || 4f99d882-04af-47bc-9e2d-a12aaecbf753 | s_rally_e4dbf0c1_LdpjrVlZ-shelved | active || 6a45e590-08fc-471f-b4d8-9a6195e88d07 | s_rally_e4dbf0c1_f3TP5nP4-shelved | active || a166585b-fb88-4c0a-a0c9-47443e9ca8be | s_rally_e4dbf0c1_lEthjCi9-shelved | active || fa680415-4448-4564-9b22-bdf2b86b6cd2 | s_rally_e4dbf0c1_nuqTKzuQ-shelved | active || 6eec3597-ee52-436c-9970-ec11415cbb4e | sdf1 | active || 1e84937a-a700-447b-8ffc-3f162127300d | snap1 | active |+--------------------------------------+-----------------------------------+--------+(rally)[root@control01 deployments]# rally deployment create --fromenv --name regiOne-99cloud+--------------------------------------+---------------------+-----------------+------------------+--------+| uuid | created_at | name | status | active |+--------------------------------------+---------------------+-----------------+------------------+--------+| 6682d272-5d99-400b-9de9-c4d46a705ba6 | 2017-05-08 03:21:46 | regiOne-99cloud | deploy-&gt;finished | |+--------------------------------------+---------------------+-----------------+------------------+--------+Using deployment: 6682d272-5d99-400b-9de9-c4d46a705ba6~/.rally/openrc was updatedHINTS:* To use standard OpenStack clients, set up your env by running: source ~/.rally/openrc OpenStack clients are now configured, e.g run: openstack image list(rally)[root@control01 deployments]# source ~/.rally/openrc### 测试deployments是否可用(rally)[root@control01 deployments]# openstack image list+--------------------------------------+-----------------------------------+--------+| ID | Name | Status |+--------------------------------------+-----------------------------------+--------+| 83dc79c8-c6df-4e23-808c-0209778be1b2 | 123 | active || 9d4cc662-bdfb-46cb-996d-3b773ee161d5 | a1 | active || 255cc63c-f635-4bbb-850c-e319d2e4c17d | cenos-raw | active || 63c5d475-e96a-4c1f-985b-12685aa2a95b | centos6.5 | active || 15ed4618-4b3b-4cc3-a9ef-6967e3a3091d | centos7 | active || 15f04241-38cc-482d-b6a3-42ba9d29f47a | cirros | active || 6491747b-2fea-4d7c-b034-b3fe5089df2c | d1 | active || 4f99d882-04af-47bc-9e2d-a12aaecbf753 | s_rally_e4dbf0c1_LdpjrVlZ-shelved | active || 6a45e590-08fc-471f-b4d8-9a6195e88d07 | s_rally_e4dbf0c1_f3TP5nP4-shelved | active || a166585b-fb88-4c0a-a0c9-47443e9ca8be | s_rally_e4dbf0c1_lEthjCi9-shelved | active || fa680415-4448-4564-9b22-bdf2b86b6cd2 | s_rally_e4dbf0c1_nuqTKzuQ-shelved | active || 6eec3597-ee52-436c-9970-ec11415cbb4e | sdf1 | active || 1e84937a-a700-447b-8ffc-3f162127300d | snap1 | active |+--------------------------------------+-----------------------------------+--------+(rally)[root@control01 deployments]# rally deployment list+--------------------------------------+---------------------+-----------------+-------------------+--------+| uuid | created_at | name | status | active |+--------------------------------------+---------------------+-----------------+-------------------+--------+| af993ae9-363e-4701-b343-626063eaf715 | 2017-04-26 07:13:50 | existing | cleanup-&gt;finished | || 6682d272-5d99-400b-9de9-c4d46a705ba6 | 2017-05-08 03:21:46 | regiOne-99cloud | deploy-&gt;finished | * |+--------------------------------------+---------------------+-----------------+-------------------+--------+]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
        <tag>rally</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[How to use tempest in kolla]]></title>
    <url>%2F2017%2F07%2F06%2Fopenstack%2Ftempest%2F</url>
    <content type="text"><![CDATA[Tempest 介绍在接触tempest之前我们应该有如下疑问。 Tempest是什么Tempest为OpenStack的功能测试、集成测试项目，它被设计为可在各种不同项目中使用。在OpenStack核心项目中的单元测试代码中经常可以看到它的身影，在一些孵化项目中也会使用Tempest去测试。它可以验证代码的正确性，已经成为OpenStack项目中不可或缺的组成部分. 主要用于什么测试？（1）API接口测试 （2）复杂场景测试 （3）单元测试 使用kolla部署tempest 在/etc/kolla/globals.yml开启tempest同时设置tempest相应的配置参数（image_id等） 注: 我是使用cirros镜像进行测试，按照实际情况替换相应参数 123456[root@control01 ~]# vim /etc/kolla/globals.ymlenable_tempest: &quot;yes&quot; tempest_image_id: &quot;570cab5e-4609-43cc-8c1e-3f7d7217ed30&quot;tempest_flavor_ref_id: &quot;1&quot;tempest_public_network_id: &quot;4c5928f2-6ade-498f-a692-98a775a7183a&quot;tempest_floating_network_name: &quot;public1&quot; 参数以如下命令获得 123456789101112131415161718192021222324252627# image id[root@control01 ~]# openstack image show cirros -c id -f value570cab5e-4609-43cc-8c1e-3f7d7217ed30# flavor_ref_id[root@control01 ~]# nova flavor-list+----+-----------+-----------+------+-----------+------+-------+-------------+-----------+| ID | Name | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public |+----+-----------+-----------+------+-----------+------+-------+-------------+-----------+| 1 | m1.tiny | 512 | 1 | 0 | | 1 | 1.0 | True || 2 | m1.small | 2048 | 20 | 0 | | 1 | 1.0 | True || 3 | m1.medium | 4096 | 40 | 0 | | 2 | 1.0 | True || 4 | m1.large | 8192 | 80 | 0 | | 4 | 1.0 | True || 5 | m1.xlarge | 16384 | 160 | 0 | | 8 | 1.0 | True# public_network_id[root@control01 ~]# openstack network show public1 -c id -f value4c5928f2-6ade-498f-a692-98a775a7183a# floating_network_name[root@control01 ~]# openstack network list+--------------------------------------+----------+--------------------------------------+| ID | Name | Subnets |+--------------------------------------+----------+--------------------------------------+| 0474116d-0fbd-41a3-ab0c-2c2d11545937 | demo-net | 7366e03d-027b-4a44-b270-d0478584f3e8 || 4c5928f2-6ade-498f-a692-98a775a7183a | public1 | 2c47f34c-f55a-4739-a677-2cbfa56b6312 |+--------------------------------------+----------+--------------------------------------+ 使用kolla-ansible deploy部署tempest 1234root@control01 ~]# kolla-ansible deploy -t tempest## 看到tempest容器运行部署就算成功root@control01 ~]# docker ps | grep tempest95b57e77b006 kolla/centos-source-tempest:5.0.0 &quot;kolla_start&quot; 46 hours ago Up 21 hours Tempest使用准备工作了解tempest cmd基本使用官网文档 123456789101112131415161718192021222324252627(tempest)[root@control01 tempest-16.0.1.dev380]# tempest --helpusage: tempest [--version] [-v | -q] [--log-file LOG_FILE] [-h] [--debug]Tempest cli applicationoptional arguments: --version show program&apos;s version number and exit -v, --verbose Increase verbosity of output. Can be repeated. -q, --quiet Suppress output except warnings and errors. --log-file LOG_FILE Specify a file to log output. Disabled by default. -h, --help Show help message and exit. --debug Show tracebacks on errors.Commands: account-generator Create accounts.yaml file for concurrent test runs. cleanup Cleanup after tempest run complete print bash completion command help print detailed help for another command init Setup a local working environment for running tempest list-plugins List all tempest plugins run Run tempest verify-config Verify your current tempest configuration workspace list Outputs the name and path of all known tempest workspaces workspace move Changes the path of a given tempest workspace --name to --path workspace register Registers a new tempest workspace via a given --name and --path workspace remove Deletes the entry for a given tempest workspace --name workspace rename Renames a tempest workspace from --old-name to --new-name 常用的命令有 12345678# 为tempest初始化working environmenttempest init# 验证配置是否正确tempest verify-config# 执行tempest测试tempest run 配置tempest.conf下面是kolla tempest.conf的模版 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273[DEFAULT]debug = &#123;&#123; openstack_logging_debug &#125;&#125;log_file = tempest.loguse_stderr = Falselog_dir = /var/log/kolla/tempest/[auth]admin_username = &#123;&#123; openstack_auth.username &#125;&#125;admin_password = &#123;&#123; keystone_admin_password &#125;&#125;admin_project_name = &#123;&#123; openstack_auth.project_name &#125;&#125;admin_domain_name = &#123;&#123; openstack_auth.domain_name &#125;&#125;[dashboard]dashboard_url = &#123;&#123; internal_protocol &#125;&#125;://&#123;&#123; kolla_internal_fqdn &#125;&#125;login_url = &#123;&#123; internal_protocol &#125;&#125;://&#123;&#123; kolla_internal_fqdn &#125;&#125;/auth/login/[service_available]cinder = &#123;&#123; enable_cinder &#125;&#125;neutron = &#123;&#123; enable_neutron &#125;&#125;glance = &#123;&#123; enable_glance &#125;&#125;swift = &#123;&#123; enable_swift &#125;&#125;nova = &#123;&#123; enable_nova &#125;&#125;heat = &#123;&#123; enable_heat &#125;&#125;horizon = &#123;&#123; enable_horizon &#125;&#125;ceilometer = &#123;&#123; enable_ceilometer &#125;&#125;[compute]max_microversion = latestimage_ref = &#123;&#123; tempest_image_id &#125;&#125;image_ref_alt = &#123;&#123; tempest_image_alt_id &#125;&#125;flavor_ref = &#123;&#123; tempest_flavor_ref_id &#125;&#125;flavor_ref_alt = &#123;&#123; tempest_flavor_ref_alt_id &#125;&#125;region = &#123;&#123; openstack_region_name &#125;&#125;[dashboard]dashboard_url = &#123;&#123; internal_protocol &#125;&#125;://&#123;&#123; kolla_internal_fqdn &#125;&#125;/login_url = &#123;&#123; internal_protocol &#125;&#125;://&#123;&#123; kolla_internal_fqdn &#125;&#125;/auth/login[identity]region = &#123;&#123; openstack_region_name &#125;&#125;auth_version = v3uri = &#123;&#123; internal_protocol &#125;&#125;://&#123;&#123; kolla_internal_fqdn &#125;&#125;:&#123;&#123; keystone_admin_port &#125;&#125;/v2.0uri_v3 = &#123;&#123; internal_protocol &#125;&#125;://&#123;&#123; kolla_internal_fqdn &#125;&#125;:&#123;&#123; keystone_admin_port &#125;&#125;/v3[image]region = &#123;&#123; openstack_region_name &#125;&#125;http_image = &#123;&#123; image_url &#125;&#125;[network]region = &#123;&#123; openstack_region_name &#125;&#125;public_network_id = &#123;&#123; tempest_public_network_id &#125;&#125;floating_network_name = &#123;&#123; tempest_floating_network_name &#125;&#125;project_networks_reachable = false[network-feature-enabled]ipv6 = false[object-storage]region = &#123;&#123; openstack_region_name &#125;&#125;[orchestration]region = &#123;&#123; openstack_region_name &#125;&#125;[volume]region = &#123;&#123; openstack_region_name &#125;&#125;[volume-feature-enabled]api_v1 = False[validation]image_ssh_user = &#123;&#123; tempest_image_ssh_user &#125;&#125;image_ssh_password = &#123;&#123; tempest_image_ssh_password &#125;&#125; 使用tempest测试api接口明确需求： 指定某些api进行测试；例如我只想测试 compute的相关api 执行所有的测试 单项api测试 去掉某些我不想测试的api 执行所有测试12345678# 验证你的配置是否正确tempest verify-config# 执行所有测试tempest run --subunit &gt; result.subunit # 生成测试报告result.htmlsubunit2html result.subunit 黑名单–blacklist-file的使用12345--blacklist-file &lt;file name&gt; Sample regex file: (^tempest\.api) # Comments about this regex tempest.scenario.test_server_basic_ops # Matches this test explicitly 例如除了tempest.scenario不测，其他的都测 1234(tempest)[root@control01 tempest-16.0.1.dev380]# cat blacklisttempest.scenario.*tempest run --blacklist-file blacklist 白名单–whitelist-file的使用123456789101112# 只测试tempest.api.compute (tempest)[root@control01 tempest-16.0.1.dev380]# cat whitefiletempest.api.compute.*tempest run --whitelist-file whitefile# 测试一个api接口(tempest)[root@control01 tempest-16.0.1.dev380]# cat one_whitefiletempest.api.compute.images.test_list_image_filters.ListImageFiltersTestJSON.test_list_images_filter_by_changes_sincetempest run --whitelist-file one_whitefile 使用ipdb调试tempest api安装ipdb 1pip install ipdb 在程序需要中断的地方插入 1import ipdb; ipdb.set_trace() 运行程序后, 会在执行到set_trace()的时候中断程序 并出现提示符 (ipdb) … 这时输入help即可看到ipdb下常用的命令啦 比较常用的是看看当前的变量 a 以及下一步 n 还有就是 dir() 方法 可以查看一个对象有那些方法可以调用 ipdb比pdb的强大在于 他包含啦 ipython 特性. 可以支持tab补全 下面ipdb调试tempest api接口的一个例子 123456789101112131415161718192021222324vim tempest/api/compute/images/test_list_image_filters.py @decorators.idempotent_id(&apos;a3f5b513-aeb3-42a9-b18e-f091ef73254d&apos;) def test_list_images_filter_by_status(self): # The list of images should contain only images with the # provided status import ipdb; ipdb.set_trace() params = &#123;&apos;status&apos;: &apos;ACTIVE&apos;&#125; images = self.client.list_images(**params)[&apos;images&apos;] self.assertNotEmpty([i for i in images if i[&apos;id&apos;] == self.image1_id]) self.assertNotEmpty([i for i in images if i[&apos;id&apos;] == self.image2_id]) self.assertNotEmpty([i for i in images if i[&apos;id&apos;] == self.image3_id])## 进行调试(tempest)[root@control01 tempest-16.0.1.dev380]# python -m testtools.run tempest.api.compute.images.test_list_image_filters.ListImageFiltersTestJSON.test_list_images_filter_by_changes_sinceTests running.../var/lib/kolla/venv/lib/python2.7/site-packages/IPython/core/debugger.py:243: DeprecationWarning: The `color_scheme` argument is deprecated since version 5.1 DeprecationWarning)&gt; /tempest-source/tempest-16.0.1.dev380/tempest/api/compute/images/test_list_image_filters.py(100)resource_setup() 99 # Create instances and snapshots via nova--&gt; 100 cls.server1 = cls.create_test_server() 101ipdb&gt; 测试脚本下面是一个简单的测试脚本，测试基本都是通过的，在控制节点执行这个脚本就ok； 报告生成在 ／tmp/tempest目录下。 123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@control01 kolla-test]# cat run_tempest.sh#!bin/bashTEMPEST_PATH=&quot;/tmp/tempest&quot;TEMPEST_FILE=&quot;/tmp/tempest/results.html&quot;if [ ! -d &quot;$TEMPEST_PATH&quot; ]; then mkdir &quot;$TEMPEST_PATH&quot;fiif [ -f &quot;$TEMPEST_FILE&quot; ]; then rm -f &quot;$TEMPEST_FILE&quot;ficat &lt;&lt; EOF &gt; &quot;blacklist&quot;tempest.api.compute.images.test_list_image_filters.ListImageFiltersTestJSONtempest.api.network.admin.test_metering_extensions.MeteringTestJSONtempest.api.network.test_extensions.ExtensionsTestJSONtempest.api.volume.test_volume_delete_cascade.VolumesDeleteCascadetempest.api.compute.admin.test_auto_allocate_network.AutoAllocateNetworkTesttempest.api.compute.servers.test_list_server_filters.ListServerFiltersTestJSONtempest.api.compute.admin.test_server_diagnostics.ServerDiagnosticsV248Testtempest.api.compute.images.test_images.ImagesTestJSONtempest.api.compute.servers.test_delete_server.DeleteServersTestJSONtempest.api.compute.volumes.test_attach_volume.AttachVolumeShelveTestJSONtempest.api.compute.images.test_images_oneserver.ImagesOneServerTestJSONtempest.api.compute.servers.test_server_actions.ServerActionsTestJSONtempest.api.compute.servers.test_servers_negative.ServersNegativeTestJSONtempest.api.volume.admin.test_volume_types.VolumeTypesTesttempest.api.volume.admin.test_volumes_backup.VolumesBackupsAdminTesttempest.api.volume.test_volume_delete_cascade.VolumesDeleteCascadetempest.api.volume.test_volumes_backup.VolumesBackupsTesttempest.api.volume.test_volumes_snapshots.VolumesSnapshotTestJSONtempest.scenario.test_minimum_basic.TestMinimumBasicScenariotempest.scenario.test_shelve_instance.TestShelveInstancetempest.scenario.test_snapshot_pattern.TestSnapshotPatterntempest.scenario.test_volume_boot_pattern.TestVolumeBootPatterntempest.scenario.test_security_groups_basic_ops.TestSecurityGroupsBasicOpstempest.scenario.test_encrypted_cinder_volumes.TestEncryptedCinderVolumesEOFdocker cp blacklist tempest:/tempest/blacklistdocker exec tempest bash -c &apos;tempest run --config-file /etc/tempest/tempest.conf --blacklist-file /tempest/blacklist --subunit &gt; /tmp/results.subunit&apos;docker exec tempest bash -c &apos;subunit2html /tmp/results.subunit /tmp/results.html&apos;docker cp tempest:/tmp/results.html $TEMPEST_FIL]]></content>
      <categories>
        <category>Openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
        <tag>tempest</tag>
      </tags>
  </entry>
</search>