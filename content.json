{"meta":{"title":"Zhubingbing","subtitle":null,"description":null,"author":"zhubingbing","url":"http://yoursite.com/child"},"pages":[{"title":"分类","date":"2017-07-03T07:04:05.000Z","updated":"2017-07-03T07:05:14.000Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/child/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2017-07-03T02:52:02.000Z","updated":"2017-07-03T02:53:08.000Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/child/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"","slug":"go/go-01","date":"2017-11-09T11:43:24.000Z","updated":"2017-11-09T11:43:24.000Z","comments":true,"path":"2017/11/09/go/go-01/","link":"","permalink":"http://yoursite.com/child/2017/11/09/go/go-01/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"","slug":"pouch/pouch-01","date":"2017-11-09T06:56:17.000Z","updated":"2017-11-09T06:56:17.000Z","comments":true,"path":"2017/11/09/pouch/pouch-01/","link":"","permalink":"http://yoursite.com/child/2017/11/09/pouch/pouch-01/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"","slug":"kubernetes/docker","date":"2017-11-07T10:32:29.000Z","updated":"2017-11-07T10:32:29.000Z","comments":true,"path":"2017/11/07/kubernetes/docker/","link":"","permalink":"http://yoursite.com/child/2017/11/07/kubernetes/docker/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"opencord","slug":"opencord/install_opencord","date":"2017-10-30T08:30:00.000Z","updated":"2018-03-16T08:02:49.000Z","comments":true,"path":"2017/10/30/opencord/install_opencord/","link":"","permalink":"http://yoursite.com/child/2017/10/30/opencord/install_opencord/","excerpt":"","text":"opencord安装安装参考link: https://guide.opencord.org/quickstarts.html我们采用 CORD-in-a-Box 场景去搭建opencord，如果没有资源可以在 https://www.cloudlab.us/ 申请资源cloudlab是和cord社区合作的 12345678910111213curl -o ~/cord-bootstrap.sh https://raw.githubusercontent.com/opencord/cord/master/scripts/cord-bootstrap.shchmod +x cord-bootstrap.sh./cord-bootstrap.sh -vcd ~/cord/build &amp;&amp; \\make PODCONFIG=rcord-virtual.yml config &amp;&amp; \\make -j4 build |&amp; tee ~/build.out &amp;&amp; \\make pod-test |&amp; tee ~/test.out 问题1. 在ubuntu14.04上vagrant 安装libvirt plugin一直提示报错报错如下1extconf.rb:73:in `&lt;main&gt;&apos;: libvirt library not found in default locations (RuntimeError) 解决方案1CONFIGURE_ARGS=&apos;with-ldflags=-L/opt/vagrant/embedded/lib with-libvirt-include=/usr/include/libvirt with-libvirt-lib=/usr/lib&apos; GEM_HOME=~/.vagrant.d/gems GEM_PATH=$GEM_HOME:/opt/vagrant/embedded/gems PATH=/opt/vagrant/embedded/bin:$PATH vagrant plugin install vagrant-libvirt 问题2. vagrant 安装box时间太长，国内源太慢解决方案：121. 下载box，然后导入2. 翻墙 问题3. qemu-img was not found in your path执行vagrant mutate ubuntu/trusty64 libvirt –input-provider virtualbox报错qemu-img was not found in your path 解决方案1apt-get install qemu-utils 问题4 ERROR: Unable to find an inventory file, specify one with -i ?执行ansible-playbook -i inventory/localhost devel-tools-playbook.yml报错1ERROR: Unable to find an inventory file, specify one with -i ? 解决方案1找到localhost文件，换成rcode makefile主要的工作如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596971. prereqs-check:2. build-local-bootstrap:3. ciab-ovs:4. vagrant-up:5. vagrant-ssh-install:6. config-ssh-key:7. copy-cord:8. cord-config:9. prep-buildnode10. prep-headnode:11. deploy-elasticstack12. prep-computenode:# MaaS targets13. build-maas-images:14. maas-prime:15. publish-maas-images:16. deploy-maas:# ONOS targets17. build-onos-apps:18. publish-onos-apps:19. deploy-mavenrepo:20. deploy-onos:21. onos-debug:# XOS targets22. docker-images:23. core-image:# Requires ib_actions.yml file which is on the build host24. publish-docker-images:25. start-xos:26. onboard-profile# OpenStack targets27. glance-images:28. deploy-openstack:29. deploy-computenode:30. onboard-openstack:# Post-onboarding targets31. setup-automation:# Additional CiaB targets32. setup-ciab-pcu:33. computes-up: setup-ciab-pcu33. refresh-fabric:# Testing targets34. pod-test:35. mcord-test:36. mcord-spirent-test:37. fabric-pingtest:# Local Targets, bring up XOS containers without a VM38. local-cord-config:39. local-docker-images:40. local-core-image:41. local-start-xos:42. local-onboard-profile: does文档 1. configure 生成所需要配置文件生成配置文件 主要代码123456789101112config: $(CONFIG_FILES) @echo &quot;&quot; @echo &quot;CORD is configured with profile: &apos;$(PROFILE)&apos;, scenario: &apos;$(SCENARIO)&apos;&quot; @echo &quot;Run &apos;make -j4 build&apos; to continue.&quot;$(CONFIG_FILES): test -e &quot;$(PODCONFIG_PATH)&quot; || &#123; echo &quot;PODCONFIG file $(PODCONFIG_PATH) doesn&apos;t exist!&quot; ; exit 1; &#125; ansible-playbook -i &apos;localhost,&apos; --extra-vars=&quot;cord_podconfig=&apos;$(PODCONFIG_PATH)&apos; genconfig_dir=&apos;$(GENCONFIG_D)&apos; scenarios_dir=&apos;$(SCENARIOS_D)&apos;&quot; $(BUILD)/ansible/genconfig.yml $(LOGCMD)printconfig: @echo &quot;Scenario: &apos;$(SCENARIO)&apos;&quot; @echo &quot;Profile: &apos;$(PROFILE)&apos;&quot;","categories":[{"name":"opencord","slug":"opencord","permalink":"http://yoursite.com/child/categories/opencord/"}],"tags":[{"name":"sdn","slug":"sdn","permalink":"http://yoursite.com/child/tags/sdn/"},{"name":"opencord","slug":"opencord","permalink":"http://yoursite.com/child/tags/opencord/"}]},{"title":"ansible openstack","slug":"openstack/openstack02","date":"2017-10-22T11:30:00.000Z","updated":"2017-11-01T08:46:43.000Z","comments":true,"path":"2017/10/22/openstack/openstack02/","link":"","permalink":"http://yoursite.com/child/2017/10/22/openstack/openstack02/","excerpt":"","text":"目的 使用ansible 快速部署openstack poc环境。 熟悉ansible 操作openstack ###","categories":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/child/categories/openstack/"}],"tags":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/child/tags/openstack/"},{"name":"ansible","slug":"ansible","permalink":"http://yoursite.com/child/tags/ansible/"}]},{"title":"期货做盘-1","slug":"study/busyness-1","date":"2017-10-18T03:08:00.000Z","updated":"2017-11-01T08:46:53.000Z","comments":true,"path":"2017/10/18/study/busyness-1/","link":"","permalink":"http://yoursite.com/child/2017/10/18/study/busyness-1/","excerpt":"","text":"期货交易交易方法： 趋势背离买卖点 平台买卖点 均线买卖点 按照级别划分 “定理”，大级别&lt;小级别 某个级别的背离，肯定有该级别的单波上涨。 到了大级别的走势的顶点或者低点会产生级别共振。级别共振是一个操作点。 走势确认的买卖点","categories":[{"name":"交易","slug":"交易","permalink":"http://yoursite.com/child/categories/交易/"}],"tags":[{"name":"期货","slug":"期货","permalink":"http://yoursite.com/child/tags/期货/"}]},{"title":"openshift-离线环境部署","slug":"openshift/iso-ready","date":"2017-10-09T08:30:00.000Z","updated":"2017-11-01T08:46:37.000Z","comments":true,"path":"2017/10/09/openshift/iso-ready/","link":"","permalink":"http://yoursite.com/child/2017/10/09/openshift/iso-ready/","excerpt":"","text":"前言之前制作了一个不够智能，随着版本的升级又要重新制作，很烦。 思考做openshift iso 需要准备的点; docker-registry 相关的镜像，openshift 要用到的相关镜像 openshift 的rpm包，也就是仓库的搭建 版本更迭，如何能够快速制作相应版本的iso 后期如何维护，维护成本 docker-registry 相关的镜像因为很多镜像是国外的镜像，所以需要翻墙下载，可以使用这个哥们的代理12345gcr.mritd.me#使用方法resgistry_url=&quot;my_registry&quot;dockeer pull gcr.mritd.me/google_containers/heapster:v1.2.0docker tag gcr.mritd.me/google_containers/heapster:v1.2.0 $resgistry_url/google_containers/heapster:v1.2.0 需要考虑2点： docker hub速度很慢，要设置docekr 加速器，可以用shell脚本设置下 docker配置项，没多大难度。 docker pull 是否能使用python实现(并发)，考虑实现成本，镜像最多也不过20个，所以还是用shell脚本实现。 repo 仓库搭建repo仓库搭建要考虑考虑几点。 版本的对应，openshift要用到的rpm包大约300多M。所以维护的基本的openshift的repo源。 后期维护，例如如果要添加几个repo包，如何快速添加，同步repo包如何快速同步。一种思路就是把openshift 官方repo源同步下来，很大。大约1个多G第二种思路就是自己安装一遍，要所需要的rpm down下来制作。很小，比较麻烦。 自动智能希望能做成，选好相应的版本。就能出来一个iso。所以各个模块功能必须分明。 制作repo 源思路 使用docker容器化repo源，数据跟程序分离。12345678910111213141516171819202122## 设置yum repo[root@control01 ~]# cat /etc/yum.repos.d/CentOS-OpenShift-Origin.repo[centos-openshift-origin]name=CentOS OpenShift Originbaseurl=http://mirror.centos.org/centos/7/paas/x86_64/openshift-origin/enabled=1gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-SIG-PaaS## 查看设置yum repoyum repolist使用reposync同步yum源reposync -r centos-openshift-origin -p /test/## 使用createrepo 创建repo源createrepo -p centos-openshift-origin/## 如果我们添加了新包，需要更新yum 源createrepo --update centos-openshift-origin/ 编写yum repo dockerfile1234567891011121314151617181920212223FROM centos:centos7.1.1503MAINTAINER zhubingbing &quot;1392607554@qq.com&quot;ENV TZ &quot;Asia/Shanghai&quot;ENV TERM xterm#RUN mkdir /etc/yum.repos.d/bak &amp;&amp; cd /etc/yum.repos.d/ &amp;&amp; mv /etc/yum.repos.d/*.repo bak/#ADD cdrom.repo /etc/yum.repos.d/cdrom.repoRUN yum install -y httpd vim-minimalRUN yum clean all#RUN mv /etc/yum.repos.d/bak/* /etc/yum.repos.d/RUN sed -i &apos;s/#ServerName www.example.com:80/ServerName 127.0.0.1:80/&apos; /etc/httpd/conf/httpd.confEXPOSE 80ENTRYPOINT [&quot;/usr/sbin/httpd&quot;, &quot;-D&quot;, &quot;FOREGROUND&quot;]USER root## build imagedocker build -t yum-repo:v1 .## 启动yum 源注意: centos-openshift-origin 目录就是我们repodocker run -it -p 8040:80 --privileged --name web-yum -d -v /root/zhu/rpm-repo/centos-openshift-origin:/var/www/html/centos-openshift-origin yum-repo:v1 制作docker仓库1234567891011121314151617181920212223## 启动docker仓库docker run -d -p 4200:5000 -v /opt/registry/:/var/lib/registry --restart=always --name registry registry:2## 下载openshift 基础镜像docker pull openshift/origin-docker-registry:v3.6.0docker pull openshift/origin-sti-builder:v3.6.0docker pull openshift/origin-haproxy-router:v3.6.0docker pull openshift/origin-pod:v3.6.0docker pull openshift/origin-deployer:v3.6.0resgistry_url=192.168.199.200:4200docker tag openshift/origin-docker-registry:v3.6.0 $resgistry_url/openshift/origin-docker-registry:v3.6.0docker tag openshift/origin-sti-builder:v3.6.0 $resgistry_url/openshift/origin-sti-builder:v3.6.0docker tag openshift/origin-haproxy-router:v3.6.0 $resgistry_url/openshift/origin-haproxy-router:v3.6.0docker tag openshift/origin-pod:v3.6.0 $resgistry_url/openshift/origin-pod:v3.6.0docker tag openshift/origin-deployer:v3.6.0 $resgistry_url/openshift/origin-deployer:v3.6.0##依次pushdocker push $resgistry_url/openshift/origin-docker-registry:v3.6.0docker push $resgistry_url/openshift/origin-sti-builder:v3.6.0docker push $resgistry_url/openshift/origin-haproxy-router:v3.6.0docker push $resgistry_url/openshift/origin-pod:v3.6.0docker push $resgistry_url/openshift/origin-deployer:v3.6.0 部署","categories":[{"name":"openshift","slug":"openshift","permalink":"http://yoursite.com/child/categories/openshift/"}],"tags":[{"name":"openshift","slug":"openshift","permalink":"http://yoursite.com/child/tags/openshift/"},{"name":"pass","slug":"pass","permalink":"http://yoursite.com/child/tags/pass/"}]},{"title":"open","slug":"study/study-plan","date":"2017-09-25T08:30:00.000Z","updated":"2017-11-01T08:46:31.000Z","comments":true,"path":"2017/09/25/study/study-plan/","link":"","permalink":"http://yoursite.com/child/2017/09/25/study/study-plan/","excerpt":"","text":"","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/child/categories/kubernetes/"}],"tags":[{"name":"pass","slug":"pass","permalink":"http://yoursite.com/child/tags/pass/"},{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/child/tags/kubernetes/"}]},{"title":"测试验证k8s功能","slug":"kubernetes/kubernete-01","date":"2017-09-25T08:30:00.000Z","updated":"2017-09-25T13:52:53.000Z","comments":true,"path":"2017/09/25/kubernetes/kubernete-01/","link":"","permalink":"http://yoursite.com/child/2017/09/25/kubernetes/kubernete-01/","excerpt":"","text":"k8s功能点 容器调度策略管理, 能够指定app部署在指定目标服务器。 容器节点的创建或销毁 容器(应用)按需求水平扩展 健康状态检查与异常恢复 容器的资源管理与限制 服务及应用管理 负载均衡 应用配置的管理 指定app部署在指定目标服务器1、查看node的labels, k8s主要是通过labels来实现多维度的资源分组管理功能。123456[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f fedora]# kubectl get node --show-labelsNAME STATUS AGE LABELSte-ipopai2hb7-0-edce6decnmfd-kube-minion-guvv4egh2oeo Ready 4d beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/zone=nova,kubernetes.io/hostname=te-ipopai2hb7-0-edce6decnmfd-kube-minion-guvv4egh2oeote-ipopai2hb7-1-t7xfovwwvv2r-kube-minion-yih3r5xg2pay Ready 1h beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/zone=nova,kubernetes.io/hostname=te-ipopai2hb7-1-t7xfovwwvv2r-kube-minion-yih3r5xg2payte-ipopai2hb7-2-snxnzgqn4unf-kube-minion-jurzrnmkamjb Ready 1h beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/zone=nova,kubernetes.io/hostname=te-ipopai2hb7-2-snxnzgqn4unf-kube-minion-jurzrnmkamjbte-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f Ready,SchedulingDisabled 4d beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f 2、使用nodeSelector，选定node节点 123456789101112131415161718192021222324252627282930313233example git:(master) ✗ cat hello.yamlapiVersion: v1kind: Podmetadata: name: hello-nginx labels: app: nginxspec: containers: - name: hello-ngix image: 172.16.130.151:4000/nginx imagePullPolicy: IfNotPresent ports: - containerPort: 80 livenessProbe: tcpSocket: port: 80 restartPolicy: Always nodeSelector: kubernetes.io/hostname: te-ipopai2hb7-0-edce6decnmfd-kube-minion-guvv4egh2oeo---apiVersion: v1kind: Servicemetadata: name: hello-nginx-servicespec: type: NodePort sessionAffinity: ClientIP selector: app: nginx ports: - port: 80 nodePort: 30080 3、 创建hello.yaml, 如下可以看出部署在te-ipopai2hb7-0-edce6decnmfd-kube-minion-guvv4egh2oeo节点上1234kubectl create -f hello.yaml[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f fedora]# kubectl get pods hello-nginx -o wideNAME READY STATUS RESTARTS AGE IP NODEhello-nginx 1/1 Running 0 32m 10.100.59.5 te-ipopai2hb7-0-edce6decnmfd-kube-minion-guvv4egh2oeo 容器节点上的任务创建或销毁上面已经部署了hello-nginx pod, 可以使用以下命令进行销毁121. kubectl delete -f hello.yaml2. kubectl delete pod hello-nginx 容器(应用)按需求水平扩展我们在生产系统中会遇到某个服务需要扩缩容的场景，k8s利用rc的scale机制来完成这些工作。 创建rc-nginx12345678910111213141516171819example git:(master) ✗ cat hello-rc.yamlapiVersion: v1kind: ReplicationControllermetadata: name: rc-nginxspec: replicas: 2 template: metadata: labels: app: rc-ginx spec: containers: - name: nginx-2 image: 172.16.130.151:4000/nginx ports: - containerPort: 80 kubectl create -f hello-rc.yaml 使用scale 扩容123456789101112[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f fedora]# kubectl scale rc rc-nginx --replicas=3replicationcontroller &quot;rc-nginx&quot; scaled[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f fedora]# kubectl get rcNAME DESIRED CURRENT READY AGErc-nginx 3 3 3 3m[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f fedora]# kubectl get podNAME READY STATUS RESTARTS AGEhello-nginx 1/1 Running 0 2hnginx 1/1 Running 0 4drc-nginx-2r19p 1/1 Running 0 3mrc-nginx-79553 1/1 Running 0 10src-nginx-c6k7f 1/1 Running 0 3m 使用scale 缩容12345678910[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f fedora]# kubectl scale rc rc-nginx --replicas=1replicationcontroller &quot;rc-nginx&quot; scaled[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f fedora]# kubectl get rcNAME DESIRED CURRENT READY AGErc-nginx 1 1 1 6m[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f fedora]# kubectl get podNAME READY STATUS RESTARTS AGEhello-nginx 1/1 Running 0 2hnginx 1/1 Running 0 4drc-nginx-c6k7f 1/1 Running 0 6m 健康状态检查与异常恢复k8s pod 的健康检查主要分为2类: livenesssProbe 探针: 用于判断容器是否存活(Running状态),如果livenesssProbe探针探测到容器不健康，则使用kubelet杀掉该容器，并根据容器的重启策略做出相应的处理。如果pod不包含livenesssProbe探针，那么kubelet认为该容器livenesssProbe探针返回值永远是success。 readinessProbe 探针: 用于判断容器是否启动完成(ready 状态), 可以接受请求. 如果readinessProbe探针检测到失败,则pod的状态将会被修改。 endpoint controller将从service的endpoint 中删除包含该容器所在pod的endpoint. 使用livenesssProbe\b探针(TcpSocketAction)\b通过容器的ip地址和端口号执行tcp检查，如果能够建立tcp链接，则表明健康12345678910111213141516171819apiVersion: v1kind: Podmetadata: name: pod-with-healthcheck labels: app: nginxspec: containers: - name: hello-ngix image: 172.16.130.151:4000/nginx imagePullPolicy: IfNotPresent ports: - containerPort: 80 livenessProbe: tcpSocket: port: 80 initialDelaySeconds: 15 timeoutSeconds: 1 restartPolicy: Always 异常恢复主要是通过rc完成的，下面删除rc-nginx中的一个pod，rc检测到系统没有期望的pod数量，rc就会启动一个pod来满足期望的数量1234567891011121314151617181920[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f fedora]# kubectl get podNAME READY STATUS RESTARTS AGEhello-nginx 1/1 Running 0 2hnginx 1/1 Running 0 4dpod-with-healthcheck 1/1 Running 0 4mrc-nginx-c6k7f 1/1 Running 0 34m[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f fedora]# kubectl delete pod rc-nginx-c6k7fpod &quot;rc-nginx-c6k7f&quot; deleted[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f fedora]# kubectl get podNAME READY STATUS RESTARTS AGEhello-nginx 1/1 Running 0 2hnginx 1/1 Running 0 4dpod-with-healthcheck 1/1 Running 0 4mrc-nginx-f0v5f 0/1 ContainerCreating 0 2s[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f fedora]# kubectl get podNAME READY STATUS RESTARTS AGEhello-nginx 1/1 Running 0 2hnginx 1/1 Running 0 4dpod-with-healthcheck 1/1 Running 0 4mrc-nginx-f0v5f 1/1 Running 0 6s 容器的资源管理和限制设置资源限制（QOS）的原因如果未做过节点 nodeSelector，亲和性（node affinity）或pod亲和、反亲和性（pod affinity/anti-affinity）等Pod高级调度策略设置，我们没有办法指定服务部署到指定机器上，如此可能会造成cpu或内存等密集型的pod同时分配到相同Node，造成资源竞争。另一方面，如果未对资源进行限制，一些关键的服务可能会因为资源竞争因OOM（Out of Memory）等原因被kill掉，或者被限制CPU使用。 资源需求（Requests）和限制（ Limits）对于每一个资源，container可以指定具体的资源需求（requests）和限制（limits），requests申请范围是0到node节点的最大配置，而limits申请范围是requests到无限，即0 &lt;= requests &lt;=Node Allocatable, requests &lt;= limits &lt;= Infinity。对于CPU，如果pod中服务使用CPU超过设置的limits，pod不会被kill掉但会被限制。如果没有设置limits，pod可以使用全部空闲的cpu资源。对于内存，当一个pod使用内存超过了设置的limits，pod中container的进程会被kernel因OOM kill掉。当container因为OOM被kill掉时，系统倾向于在其原所在的机器上重启该container或本机或其他重新创建一个pod。 example-112345678910111213141516apiVersion: v1kind: Podmetadata: name: pod-with-healthcheck labels: app: nginxspec: containers: - name: hello-ngix image: 172.16.130.151:4000/nginx imagePullPolicy: IfNotPresent ports: - containerPort: 80 resources: restartPolicy: Always 服务及应用管理首先使用nginx-deployment.yaml文件创建一个Nginx Deployment，文件内容如图所示： 首先创建两个运行Nginx服务的Pod,待Pod运行之后查看一下它们的IP，并在k8s集群内通过podIP和containerPort来访问Nginx服务 12345678910111213141516171819202122232425262728293031apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx-deploymentspec: replicas: 2 template: metadata: labels: app: nginx-1 spec: containers: - name: nginx image: 172.16.130.151:4000/nginx ports: - containerPort: 80---apiVersion: v1kind: Servicemetadata: name: nginx-service labels: app: nginx-1spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: app: nginx-1 type: LoadBalancer 1234查看pod ip[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f fedora]# kubectl get pods -o yaml -l app=nginx-1 | grep podIP podIP: 10.100.83.3 podIP: 10.100.81.5 在集群内访问Nginx服务1234567891011121314151617181920212223242526[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f fedora]# curl 10.100.83.3:80&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html 使用Service发现服务创建之后，仍需要获取Service的Cluster-IP，再结合Port访问Nginx服务。 Service可以将pod IP封装起来，即使Pod发生重建，依然可以通过Service来访问Pod提供的服务。此外，Service还解决了负载均衡的问题，大家可以多访问几次Service，然后通过kubectl logs 来查看两个Nginx Pod的访问日志来确认。1234## 获取ip[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f fedora]# kubectl get service nginx-serviceNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEnginx-service 10.254.145.0 10.0.0.16 80:32182/TCP 10m 在集群内访问Service：1234567891011121314151617181920212223242526272829[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f fedora]# kubectl get service nginx-serviceNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEnginx-service 10.254.145.0 10.0.0.16 80:32182/TCP 10m[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f fedora]# curl 10.254.145.0&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; service 负载均衡,从下面日志看，通过对service的请求，均衡分发到pod上12345678910111213[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f fedora]# kubectl logs nginx-deployment-1286323699-3qsj510.0.0.7 - - [25/Sep/2017:10:01:42 +0000] &quot;GET / HTTP/1.1&quot; 200 612 &quot;-&quot; &quot;curl/7.51.0&quot; &quot;-&quot;10.0.0.7 - - [25/Sep/2017:10:02:40 +0000] &quot;GET / HTTP/1.1&quot; 200 612 &quot;-&quot; &quot;curl/7.51.0&quot; &quot;-&quot;10.0.0.7 - - [25/Sep/2017:10:04:48 +0000] &quot;GET / HTTP/1.1&quot; 200 612 &quot;-&quot; &quot;curl/7.51.0&quot; &quot;-&quot;10.0.0.7 - - [25/Sep/2017:10:07:30 +0000] &quot;GET / HTTP/1.1&quot; 200 612 &quot;-&quot; &quot;curl/7.51.0&quot; &quot;-&quot;10.0.0.7 - - [25/Sep/2017:10:07:33 +0000] &quot;GET / HTTP/1.1&quot; 200 612 &quot;-&quot; &quot;curl/7.51.0&quot; &quot;-&quot;10.0.0.7 - - [25/Sep/2017:10:08:24 +0000] &quot;GET / HTTP/1.1&quot; 200 612 &quot;-&quot; &quot;curl/7.51.0&quot; &quot;-&quot;10.0.0.7 - - [25/Sep/2017:10:08:25 +0000] &quot;GET / HTTP/1.1&quot; 200 612 &quot;-&quot; &quot;curl/7.51.0&quot; &quot;-&quot;[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f fedora]# kubectl logs nginx-deployment-1286323699-j695f10.0.0.7 - - [25/Sep/2017:10:07:29 +0000] &quot;GET / HTTP/1.1&quot; 200 612 &quot;-&quot; &quot;curl/7.51.0&quot; &quot;-&quot;10.0.0.7 - - [25/Sep/2017:10:07:31 +0000] &quot;GET / HTTP/1.1&quot; 200 612 &quot;-&quot; &quot;curl/7.51.0&quot; &quot;-&quot;10.0.0.7 - - [25/Sep/2017:10:07:32 +0000] &quot;GET / HTTP/1.1&quot; 200 612 &quot;-&quot; &quot;curl/7.51.0&quot; &quot;-&quot;10.0.0.7 - - [25/Sep/2017:10:07:32 +0000] &quot;GET / HTTP/1.1&quot; 200 612 &quot;-&quot; &quot;curl/7.51.0&quot; &quot;-&quot; 使用LoadBalancer访问服务这里使用neutron lbass作为负载均衡，这时候我们可以去界面（horizon）上查看下lb创建情况 创建成功我们给他分配一个浮动ip通过浮动ip访问应用","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/child/categories/kubernetes/"}],"tags":[{"name":"pass","slug":"pass","permalink":"http://yoursite.com/child/tags/pass/"},{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/child/tags/kubernetes/"}]},{"title":"magnum调研02","slug":"openstack/magnum/magnum-02","date":"2017-09-21T11:30:00.000Z","updated":"2017-09-21T12:08:09.000Z","comments":true,"path":"2017/09/21/openstack/magnum/magnum-02/","link":"","permalink":"http://yoursite.com/child/2017/09/21/openstack/magnum/magnum-02/","excerpt":"","text":"问题通过之前初步调研，magnum确实能把k8s启动起来。但是k8s一些组件都需要自己手动安装，这点很不好。然后我查看代码，发现最新的ocata代码已经支持heat 部署k8s的dns、dashboard等。monitor在pike版本代码也已经支持。 看到这里有点小激动啊，那说明我用magnum一键能部署一个带高级功能的k8s平台。 准备这些内容在 magnum-调研01可以查看 1、 准备registry，因为要离线安装（主要gcr.io被墙了），所有搭建自己的registry 12345678910111213141516171819## 启动私有的registrydocker run -d -p 4000:5000 -v /opt/registry:/var/lib/registry --restart=always --name registry registry:2## 所需镜像google_containers/kube-ui:v4google_containers/pause-amd64:3.0google_containers/hyperkube:v1.5.3google_containers/pause:0.8.0google_containers/kubedns-amd64:1.9google_containers/dnsmasq-metrics-amd64:1.0google_containers/kube-dnsmasq-amd64:1.4google_containers/exechealthz-amd64:1.2google_containers/defaultbackend:1.0google_containers/nginx-ingress-controller:0.9.0-beta.11## 上传镜像（可以简单写个shell脚本自动去做）registry_url=192.168.21.10:5000docker tag gcr.io/google_containers/kube-ui:v4 $registry_url/google_containers/kube-ui:v4docker push $registry_url/google_containers/kube-ui:v4 2、fedora镜像 123456789101112## 下载fedora镜像wget https://fedorapeople.org/groups/magnum/fedora-atomic-latest.qcow2## 上传镜像openstack image create \\ --disk-format=qcow2 \\ --container-format=bare \\ --file=fedora-atomic-latest.qcow2 \\ --property os_distro=&apos;fedora-atomic&apos; \\ fedora-atomic-latest 使用kolla部署magnum注: 这里不详细介绍如何使用kolla部署magnum了,kolla-ansible使用可以参考官网文档（https://github.com/openstack/kolla-ansible） 在/etc/kolla/globals.yml 打开magnum、neutron-lbass(因为要使用lb功能)，存储使用ceph,基础组件都打开12345[root@master ~]# cat /etc/kolla/globals.ymlenable_horizon_magnum: &quot;&#123;&#123; enable_magnum | bool &#125;&#125;&quot;enable_magnum: &quot;yes&quot;enable_neutron_lbaas: &quot;yes&quot;[root@master ~]# kolla-ansible deploy 创建magnum k8s template参数根据自己的实际情况进行调整12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758 magnum cluster-template-create kubernetes-cluster-template-1 \\ --image fedora-atomic-latest \\ --keypair mykey \\ --external-network public1 \\ --dns-nameserver 8.8.8.8 \\ --master-flavor 4C.4G \\ --flavor 4C.4G \\ --labels kube_dashboard_enabled=True,flannel_backend=host-gw \\ --coe kubernetes \\ --docker-volume-size 40 \\ --floating-ip-enabled \\ --volume-driver cinder \\ --insecure-registry 172.16.130.151:4000## 参数说明这里的参数需要注意的就是 --labels这个参数，因为文档里面没有详细介绍，像kube_dashboard_enabled，后面的监控啊，调整flannel的参数等等都需要使用这个参数## 相关代码代码路径 magnum/magnum/drivers/heat/k8s_template_def.py![](leanote://file/getImage?fileId=59c2490959398d3414000004)## 创建成功如下显示[root@master ~]# magnum cluster-template-show kubernetes-cluster-template-1+-----------------------+------------------------------------------------------------------+| Property | Value |+-----------------------+------------------------------------------------------------------+| insecure_registry | 172.16.130.151:4000 || labels | &#123;&apos;flannel_backend&apos;: &apos;host-gw&apos;, &apos;kube_dashboard_enabled&apos;: &apos;True&apos;&#125; || updated_at | - || floating_ip_enabled | True || fixed_subnet | - || master_flavor_id | 4C.4G || uuid | 669e5b3a-2cdf-42c1-9b8c-656b0f3bba6a || no_proxy | - || https_proxy | - || tls_disabled | False || keypair_id | mykey || public | False || http_proxy | - || docker_volume_size | 40 || server_type | vm || external_network_id | public1 || cluster_distro | fedora-atomic || image_id | fedora-atomic-latest || volume_driver | cinder || registry_enabled | False || docker_storage_driver | devicemapper || apiserver_port | - || name | kubernetes-cluster-template-1 || created_at | 2017-09-20T03:09:26+00:00 || network_driver | flannel || fixed_network | - || coe | kubernetes || flavor_id | 4C.4G || master_lb_enabled | False || dns_nameserver | 8.8.8.8 |+-----------------------+------------------------------------------------------------------+ 创建k8s 集群创建1个master节点和2个node节点的k8s集群 1234567891011121314151617181920212223242526magnum cluster-create --node-count 2 --master-count 1 --cluster-template k8s-cluster-template## 创建成功如下[root@master ~]# magnum cluster-show test-3+---------------------+------------------------------------------------------------+| Property | Value |+---------------------+------------------------------------------------------------+| status | CREATE_COMPLETE || cluster_template_id | 4886c5b3-2f0f-4b1a-94a4-7f7c4f6eb0d9 || node_addresses | [&apos;172.16.150.164&apos;, &apos;172.16.150.169&apos;] || uuid | 65c95601-8871-48de-8ebc-e2efe848111c || stack_id | 2ba19cad-8ed9-47ab-8340-a4f26b57e333 || status_reason | Stack CREATE completed successfully || created_at | 2017-09-20T07:20:29+00:00 || updated_at | 2017-09-20T07:24:03+00:00 || coe_version | v1.5.3 || keypair | mykey || api_address | https://172.16.150.154:6443 || master_addresses | [&apos;172.16.150.154&apos;] || create_timeout | 60 || node_count | 2 || discovery_url | https://discovery.etcd.io/d88395d69fa826a349ad7c7323977cec || master_count | 1 || container_version | 1.12.6 || name | test-3 |+---------------------+------------------------------------------------------------+ 访问k8s集群访问k8s集群有2种方式1、 登入k8s master节点能够直接使用k8s命令2、 远程访问可以使用 magnum cluster-config 生成所需的ca文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748## 直接登陆master节点访问[root@master ~]# nova list/usr/lib/python2.7/site-packages/novaclient/client.py:278: UserWarning: The &apos;tenant_id&apos; argument is deprecated in Ocata and its use may result in errors in future releases. As &apos;project_id&apos; is provided, the &apos;tenant_id&apos; argument will be ignored. warnings.warn(msg)+--------------------------------------+-------------------------------------------------------+--------+------------+-------------+-----------------------------------+| ID | Name | Status | Task State | Power State | Networks |+--------------------------------------+-------------------------------------------------------+--------+------------+-------------+-----------------------------------+| dd8d6bf1-882c-4132-b90e-fbf251a04351 | te-ipopai2hb7-0-edce6decnmfd-kube-minion-guvv4egh2oeo | ACTIVE | - | Running | private=10.0.0.9, 172.16.150.164 || 215e1f59-8270-488b-b02a-1f83f0ad8fae | te-ipopai2hb7-1-5z3wzug3hsjg-kube-minion-ktzk5ugdsclg | ACTIVE | - | Running | private=10.0.0.12, 172.16.150.169 || e45e4f94-d9d3-4b6b-8632-9437d7fa7101 | te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f | ACTIVE | - | Running | private=10.0.0.7, 172.16.150.154 || a435bda3-dbfb-4df1-b426-42f4142f1b4d | te-tcrckexviy-0-66y6o7a76osg-kube-master-yozurhnfidfw | ACTIVE | - | Running | private=10.0.0.12, 172.16.150.158 |+--------------------------------------+-------------------------------------------------------+--------+------------+-------------+-----------------------------------+[root@master ~]# ssh fedora@172.16.150.154Warning: Permanently added &apos;172.16.150.154&apos; (ECDSA) to the list of known hosts.Last login: Wed Sep 20 09:34:09 2017 from 172.16.150.163[fedora@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f ~]$[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f fedora]# kubectl get nodeNAME STATUS AGEte-ipopai2hb7-0-edce6decnmfd-kube-minion-guvv4egh2oeo Ready 3hte-ipopai2hb7-1-5z3wzug3hsjg-kube-minion-ktzk5ugdsclg Ready 3hte-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f Ready,SchedulingDisabled 3h[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f fedora]# kubectl get pods --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEdefault nginx 1/1 Running 0 1hkube-system coredns-2417336708-gv1df 1/1 Running 0 3hkube-system kube-controller-manager-te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f 1/1 Running 0 3hkube-system kube-proxy-te-ipopai2hb7-0-edce6decnmfd-kube-minion-guvv4egh2oeo 1/1 Running 0 3hkube-system kube-proxy-te-ipopai2hb7-1-5z3wzug3hsjg-kube-minion-ktzk5ugdsclg 1/1 Running 0 3hkube-system kube-proxy-te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f 1/1 Running 0 3hkube-system kube-scheduler-te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f 1/1 Running 0 3hkube-system kubernetes-dashboard-953129222-2m1fl 1/1 Running 0 3h[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f fedora]### 使用magnum cluster-config[root@master ~]# mkdir -p ~/clusters/kubernetes-cluster[root@master ~]# $(magnum cluster-config test-3 --dir ~/clusters/kubernetes-cluster)export KUBECONFIG=/home/user/clusters/kubernetes-cluster/config[root@master ~]# kubectl -n kube-system get pod --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEdefault nginx 1/1 Running 0 1hkube-system coredns-2417336708-gv1df 1/1 Running 0 3hkube-system kube-controller-manager-te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f 1/1 Running 0 3hkube-system kube-proxy-te-ipopai2hb7-0-edce6decnmfd-kube-minion-guvv4egh2oeo 1/1 Running 0 3hkube-system kube-proxy-te-ipopai2hb7-1-5z3wzug3hsjg-kube-minion-ktzk5ugdsclg 1/1 Running 0 3hkube-system kube-proxy-te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f 1/1 Running 0 3hkube-system kube-scheduler-te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f 1/1 Running 0 3hkube-system kubernetes-dashboard-953129222-2m1fl 1/1 Running 0 3h 验证k8s功能dashboard1234567891011121314151617181920212223242526272829303132333435## 查看dashboard svc[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f fedora]# kubectl get svc -n kube-systemNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEkube-dns 10.254.0.10 &lt;none&gt; 53/UDP,53/TCP 3hkubernetes-dashboard 10.254.14.9 &lt;nodes&gt; 80:30029/TCP 3h## 查看dashboard svc的定义[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f fedora]# kubectl get svc kubernetes-dashboard -n kube-system -o yamlapiVersion: v1kind: Servicemetadata: creationTimestamp: 2017-09-20T07:22:11Z labels: app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system resourceVersion: &quot;40&quot; selfLink: /api/v1/namespaces/kube-system/services/kubernetes-dashboard uid: 6af71858-9dd4-11e7-b8fc-fa163e86cfbaspec: clusterIP: 10.254.14.9 ports: - nodePort: 30029 port: 80 protocol: TCP targetPort: 9090 selector: app: kubernetes-dashboard sessionAffinity: None type: NodePortstatus: loadBalancer: &#123;&#125;从上面可以看出，dashboard service使用的是nodeport方式，那我们可以使用http://node_ip:30029 进行愉快的访问了 这就是我们k8s 的dashboard了 dnsmagnum k8s的dns服务默认使用coredns具体实现可以参看coredns的官网文档 123456789101112131415161718192021222324创建一个buybox pod[root@master example]# cat busybox.yamlapiVersion: v1kind: Podmetadata: name: busybox namespace: defaultspec: containers: - image: 172.16.130.151:4000/busybox command: - sleep - &quot;3600&quot; imagePullPolicy: IfNotPresent name: busybox restartPolicy: Alway[root@master example]# kubectl create -f busybox.yaml## 完美解析, nice[root@master example]# kubectl exec busybox nslookup kubernetesServer: 10.254.0.10Address 1: 10.254.0.10 kube-dns.kube-system.svc.cluster.localName: kubernetesAddress 1: 10.254.0.1 kubernetes.default.svc.cluster.local k8s使用neutron lb功能123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869## 启动一个test-nginx-pod[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f examples]# cat test-pod.yamlapiVersion: v1kind: Podmetadata: name: nginx labels: app: nginxspec: containers: - name: nginx image: 172.16.130.151:4000/nginx ports: - containerPort: 80[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f examples]# kubectl create -f test-pod.yamlpod &quot;nginx&quot; created## 启动test-service[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f examples]# cat test-service.yamlapiVersion: v1kind: Servicemetadata: name: nginxservice labels: app: nginxspec: ports: - port: 80 targetPort: 80 protocol: TCP selector: app: nginx type: LoadBalancer[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f examples]# kubectl create -f test-service.yamlservice &quot;nginxservice&quot; created[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f examples]# kubectl get svcNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes 10.254.0.1 &lt;none&gt; 443/TCP 4hnginxservice 10.254.84.116 10.0.0.15 80:30443/TCP 2m## 使用EXTERNAL-IP 访问服务[root@te-lgs3rgroc5-0-dvcxdlqunz3h-kube-master-sespgox2v33f examples]# curl 10.0.0.15&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html 这时候我们可以去界面（horizon）上查看下lb创建情况 创建成功我们给他分配一个浮动ip通过浮动ip访问应用","categories":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/child/categories/openstack/"}],"tags":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/child/tags/openstack/"},{"name":"magnum","slug":"magnum","permalink":"http://yoursite.com/child/tags/magnum/"}]},{"title":"openstack 通用设计思路","slug":"openstack/openstack-01","date":"2017-09-21T11:30:00.000Z","updated":"2017-10-09T06:42:50.000Z","comments":true,"path":"2017/09/21/openstack/openstack-01/","link":"","permalink":"http://yoursite.com/child/2017/09/21/openstack/openstack-01/","excerpt":"","text":"API 前端服务每个 OpenStack 组件可能包含若干子服务，其中必定有一个 API 服务负责接收客户请求。 以 Nova 为例，nova-api 作为 Nova 组件对外的唯一窗口，向客户暴露 Nova 能够提供的功能。 当客户需要执行虚机相关的操作，能且只能向 nova-api 发送 REST 请求。 这里的客户包括终端用户、命令行和 OpenStack 其他组件。 设计 API 前端服务的好处在于： 对外提供统一接口，隐藏实现细节 API 提供 REST 标准调用服务 magnum-api 服务分析","categories":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/child/categories/openstack/"}],"tags":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/child/tags/openstack/"},{"name":"magnum","slug":"magnum","permalink":"http://yoursite.com/child/tags/magnum/"}]},{"title":"zsh no_nomatch *","slug":"macos/zsh","date":"2017-09-21T04:30:00.000Z","updated":"2017-09-22T02:48:08.000Z","comments":true,"path":"2017/09/21/macos/zsh/","link":"","permalink":"http://yoursite.com/child/2017/09/21/macos/zsh/","excerpt":"","text":"在 zsh 下使用 find 命令查找指定目录下所有头文件时出现问题： 1234567## example-01find . -name *.hno matches found: *.h## eexample-02git rm -rf *no matches found: * 后来查看了一些资料才知道，这是由于zsh导致的。 具体原因： 因为zsh缺省情况下始终自己解释这个 *.h，而不会传递给 find 来解释。 解决办法： 在~/.zshrc中加入:setopt no_nomatch, 然后进行source .zshrc命令","categories":[{"name":"macos","slug":"macos","permalink":"http://yoursite.com/child/categories/macos/"}],"tags":[{"name":"zsh","slug":"zsh","permalink":"http://yoursite.com/child/tags/zsh/"}]},{"title":"How to use tempest in kolla","slug":"openstack/tempest","date":"2017-08-16T04:30:00.000Z","updated":"2017-09-22T02:20:44.000Z","comments":true,"path":"2017/08/16/openstack/tempest/","link":"","permalink":"http://yoursite.com/child/2017/08/16/openstack/tempest/","excerpt":"","text":"Tempest 介绍在接触tempest之前我们应该有如下疑问。 Tempest是什么Tempest为OpenStack的功能测试、集成测试项目，它被设计为可在各种不同项目中使用。在OpenStack核心项目中的单元测试代码中经常可以看到它的身影，在一些孵化项目中也会使用Tempest去测试。它可以验证代码的正确性，已经成为OpenStack项目中不可或缺的组成部分. 主要用于什么测试？（1）API接口测试 （2）复杂场景测试 （3）单元测试 使用kolla部署tempest 在/etc/kolla/globals.yml开启tempest同时设置tempest相应的配置参数（image_id等） 注: 我是使用cirros镜像进行测试，按照实际情况替换相应参数 123456[root@control01 ~]# vim /etc/kolla/globals.ymlenable_tempest: &quot;yes&quot;tempest_image_id: &quot;570cab5e-4609-43cc-8c1e-3f7d7217ed30&quot;tempest_flavor_ref_id: &quot;1&quot;tempest_public_network_id: &quot;4c5928f2-6ade-498f-a692-98a775a7183a&quot;tempest_floating_network_name: &quot;public1&quot; 参数以如下命令获得 123456789101112131415161718192021222324252627# image id[root@control01 ~]# openstack image show cirros -c id -f value570cab5e-4609-43cc-8c1e-3f7d7217ed30# flavor_ref_id[root@control01 ~]# nova flavor-list+----+-----------+-----------+------+-----------+------+-------+-------------+-----------+| ID | Name | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public |+----+-----------+-----------+------+-----------+------+-------+-------------+-----------+| 1 | m1.tiny | 512 | 1 | 0 | | 1 | 1.0 | True || 2 | m1.small | 2048 | 20 | 0 | | 1 | 1.0 | True || 3 | m1.medium | 4096 | 40 | 0 | | 2 | 1.0 | True || 4 | m1.large | 8192 | 80 | 0 | | 4 | 1.0 | True || 5 | m1.xlarge | 16384 | 160 | 0 | | 8 | 1.0 | True# public_network_id[root@control01 ~]# openstack network show public1 -c id -f value4c5928f2-6ade-498f-a692-98a775a7183a# floating_network_name[root@control01 ~]# openstack network list+--------------------------------------+----------+--------------------------------------+| ID | Name | Subnets |+--------------------------------------+----------+--------------------------------------+| 0474116d-0fbd-41a3-ab0c-2c2d11545937 | demo-net | 7366e03d-027b-4a44-b270-d0478584f3e8 || 4c5928f2-6ade-498f-a692-98a775a7183a | public1 | 2c47f34c-f55a-4739-a677-2cbfa56b6312 |+--------------------------------------+----------+--------------------------------------+ 使用kolla-ansible deploy部署tempest 1234root@control01 ~]# kolla-ansible deploy -t tempest## 看到tempest容器运行部署就算成功root@control01 ~]# docker ps | grep tempest95b57e77b006 kolla/centos-source-tempest:5.0.0 &quot;kolla_start&quot; 46 hours ago Up 21 hours Tempest使用准备工作了解tempest cmd基本使用官网文档 123456789101112131415161718192021222324252627(tempest)[root@control01 tempest-16.0.1.dev380]# tempest --helpusage: tempest [--version] [-v | -q] [--log-file LOG_FILE] [-h] [--debug]Tempest cli applicationoptional arguments: --version show program&apos;s version number and exit -v, --verbose Increase verbosity of output. Can be repeated. -q, --quiet Suppress output except warnings and errors. --log-file LOG_FILE Specify a file to log output. Disabled by default. -h, --help Show help message and exit. --debug Show tracebacks on errors.Commands: account-generator Create accounts.yaml file for concurrent test runs. cleanup Cleanup after tempest run complete print bash completion command help print detailed help for another command init Setup a local working environment for running tempest list-plugins List all tempest plugins run Run tempest verify-config Verify your current tempest configuration workspace list Outputs the name and path of all known tempest workspaces workspace move Changes the path of a given tempest workspace --name to --path workspace register Registers a new tempest workspace via a given --name and --path workspace remove Deletes the entry for a given tempest workspace --name workspace rename Renames a tempest workspace from --old-name to --new-name 常用的命令有 12345678# 为tempest初始化working environmenttempest init# 验证配置是否正确tempest verify-config# 执行tempest测试tempest run 配置tempest.conf下面是kolla tempest.conf的模版 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273[DEFAULT]debug = &#123;&#123; openstack_logging_debug &#125;&#125;log_file = tempest.loguse_stderr = Falselog_dir = /var/log/kolla/tempest/[auth]admin_username = &#123;&#123; openstack_auth.username &#125;&#125;admin_password = &#123;&#123; keystone_admin_password &#125;&#125;admin_project_name = &#123;&#123; openstack_auth.project_name &#125;&#125;admin_domain_name = &#123;&#123; openstack_auth.domain_name &#125;&#125;[dashboard]dashboard_url = &#123;&#123; internal_protocol &#125;&#125;://&#123;&#123; kolla_internal_fqdn &#125;&#125;login_url = &#123;&#123; internal_protocol &#125;&#125;://&#123;&#123; kolla_internal_fqdn &#125;&#125;/auth/login/[service_available]cinder = &#123;&#123; enable_cinder &#125;&#125;neutron = &#123;&#123; enable_neutron &#125;&#125;glance = &#123;&#123; enable_glance &#125;&#125;swift = &#123;&#123; enable_swift &#125;&#125;nova = &#123;&#123; enable_nova &#125;&#125;heat = &#123;&#123; enable_heat &#125;&#125;horizon = &#123;&#123; enable_horizon &#125;&#125;ceilometer = &#123;&#123; enable_ceilometer &#125;&#125;[compute]max_microversion = latestimage_ref = &#123;&#123; tempest_image_id &#125;&#125;image_ref_alt = &#123;&#123; tempest_image_alt_id &#125;&#125;flavor_ref = &#123;&#123; tempest_flavor_ref_id &#125;&#125;flavor_ref_alt = &#123;&#123; tempest_flavor_ref_alt_id &#125;&#125;region = &#123;&#123; openstack_region_name &#125;&#125;[dashboard]dashboard_url = &#123;&#123; internal_protocol &#125;&#125;://&#123;&#123; kolla_internal_fqdn &#125;&#125;/login_url = &#123;&#123; internal_protocol &#125;&#125;://&#123;&#123; kolla_internal_fqdn &#125;&#125;/auth/login[identity]region = &#123;&#123; openstack_region_name &#125;&#125;auth_version = v3uri = &#123;&#123; internal_protocol &#125;&#125;://&#123;&#123; kolla_internal_fqdn &#125;&#125;:&#123;&#123; keystone_admin_port &#125;&#125;/v2.0uri_v3 = &#123;&#123; internal_protocol &#125;&#125;://&#123;&#123; kolla_internal_fqdn &#125;&#125;:&#123;&#123; keystone_admin_port &#125;&#125;/v3[image]region = &#123;&#123; openstack_region_name &#125;&#125;http_image = &#123;&#123; image_url &#125;&#125;[network]region = &#123;&#123; openstack_region_name &#125;&#125;public_network_id = &#123;&#123; tempest_public_network_id &#125;&#125;floating_network_name = &#123;&#123; tempest_floating_network_name &#125;&#125;project_networks_reachable = false[network-feature-enabled]ipv6 = false[object-storage]region = &#123;&#123; openstack_region_name &#125;&#125;[orchestration]region = &#123;&#123; openstack_region_name &#125;&#125;[volume]region = &#123;&#123; openstack_region_name &#125;&#125;[volume-feature-enabled]api_v1 = False[validation]image_ssh_user = &#123;&#123; tempest_image_ssh_user &#125;&#125;image_ssh_password = &#123;&#123; tempest_image_ssh_password &#125;&#125; 使用tempest测试api接口明确需求： 指定某些api进行测试；例如我只想测试 compute的相关api 执行所有的测试 单项api测试 去掉某些我不想测试的api 执行所有测试12345678# 验证你的配置是否正确tempest verify-config# 执行所有测试tempest run --subunit &gt; result.subunit# 生成测试报告result.htmlsubunit2html result.subunit 黑名单–blacklist-file的使用12345--blacklist-file &lt;file name&gt; Sample regex file: (^tempest\\.api) # Comments about this regex tempest.scenario.test_server_basic_ops # Matches this test explicitly 例如除了tempest.scenario不测，其他的都测 1234(tempest)[root@control01 tempest-16.0.1.dev380]# cat blacklisttempest.scenario.*tempest run --blacklist-file blacklist 白名单–whitelist-file的使用123456789101112# 只测试tempest.api.compute(tempest)[root@control01 tempest-16.0.1.dev380]# cat whitefiletempest.api.compute.*tempest run --whitelist-file whitefile# 测试一个api接口(tempest)[root@control01 tempest-16.0.1.dev380]# cat one_whitefiletempest.api.compute.images.test_list_image_filters.ListImageFiltersTestJSON.test_list_images_filter_by_changes_sincetempest run --whitelist-file one_whitefile 使用ipdb调试tempest api安装ipdb 1pip install ipdb 在程序需要中断的地方插入 1import ipdb; ipdb.set_trace() 运行程序后, 会在执行到set_trace()的时候中断程序 并出现提示符 (ipdb) … 这时输入help即可看到ipdb下常用的命令啦 比较常用的是看看当前的变量 a 以及下一步 n 还有就是 dir() 方法 可以查看一个对象有那些方法可以调用 ipdb比pdb的强大在于 他包含啦 ipython 特性. 可以支持tab补全 下面ipdb调试tempest api接口的一个例子 123456789101112131415161718192021222324vim tempest/api/compute/images/test_list_image_filters.py @decorators.idempotent_id(&apos;a3f5b513-aeb3-42a9-b18e-f091ef73254d&apos;) def test_list_images_filter_by_status(self): # The list of images should contain only images with the # provided status import ipdb; ipdb.set_trace() params = &#123;&apos;status&apos;: &apos;ACTIVE&apos;&#125; images = self.client.list_images(**params)[&apos;images&apos;] self.assertNotEmpty([i for i in images if i[&apos;id&apos;] == self.image1_id]) self.assertNotEmpty([i for i in images if i[&apos;id&apos;] == self.image2_id]) self.assertNotEmpty([i for i in images if i[&apos;id&apos;] == self.image3_id])## 进行调试(tempest)[root@control01 tempest-16.0.1.dev380]# python -m testtools.run tempest.api.compute.images.test_list_image_filters.ListImageFiltersTestJSON.test_list_images_filter_by_changes_sinceTests running.../var/lib/kolla/venv/lib/python2.7/site-packages/IPython/core/debugger.py:243: DeprecationWarning: The `color_scheme` argument is deprecated since version 5.1 DeprecationWarning)&gt; /tempest-source/tempest-16.0.1.dev380/tempest/api/compute/images/test_list_image_filters.py(100)resource_setup() 99 # Create instances and snapshots via nova--&gt; 100 cls.server1 = cls.create_test_server() 101ipdb&gt; 测试脚本下面是一个简单的测试脚本，测试基本都是通过的，在控制节点执行这个脚本就ok； 报告生成在 ／tmp/tempest目录下。 123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@control01 kolla-test]# cat run_tempest.sh#!bin/bashTEMPEST_PATH=&quot;/tmp/tempest&quot;TEMPEST_FILE=&quot;/tmp/tempest/results.html&quot;if [ ! -d &quot;$TEMPEST_PATH&quot; ]; then mkdir &quot;$TEMPEST_PATH&quot;fiif [ -f &quot;$TEMPEST_FILE&quot; ]; then rm -f &quot;$TEMPEST_FILE&quot;ficat &lt;&lt; EOF &gt; &quot;blacklist&quot;tempest.api.compute.images.test_list_image_filters.ListImageFiltersTestJSONtempest.api.network.admin.test_metering_extensions.MeteringTestJSONtempest.api.network.test_extensions.ExtensionsTestJSONtempest.api.volume.test_volume_delete_cascade.VolumesDeleteCascadetempest.api.compute.admin.test_auto_allocate_network.AutoAllocateNetworkTesttempest.api.compute.servers.test_list_server_filters.ListServerFiltersTestJSONtempest.api.compute.admin.test_server_diagnostics.ServerDiagnosticsV248Testtempest.api.compute.images.test_images.ImagesTestJSONtempest.api.compute.servers.test_delete_server.DeleteServersTestJSONtempest.api.compute.volumes.test_attach_volume.AttachVolumeShelveTestJSONtempest.api.compute.images.test_images_oneserver.ImagesOneServerTestJSONtempest.api.compute.servers.test_server_actions.ServerActionsTestJSONtempest.api.compute.servers.test_servers_negative.ServersNegativeTestJSONtempest.api.volume.admin.test_volume_types.VolumeTypesTesttempest.api.volume.admin.test_volumes_backup.VolumesBackupsAdminTesttempest.api.volume.test_volume_delete_cascade.VolumesDeleteCascadetempest.api.volume.test_volumes_backup.VolumesBackupsTesttempest.api.volume.test_volumes_snapshots.VolumesSnapshotTestJSONtempest.scenario.test_minimum_basic.TestMinimumBasicScenariotempest.scenario.test_shelve_instance.TestShelveInstancetempest.scenario.test_snapshot_pattern.TestSnapshotPatterntempest.scenario.test_volume_boot_pattern.TestVolumeBootPatterntempest.scenario.test_security_groups_basic_ops.TestSecurityGroupsBasicOpstempest.scenario.test_encrypted_cinder_volumes.TestEncryptedCinderVolumesEOFdocker cp blacklist tempest:/tempest/blacklistdocker exec tempest bash -c &apos;tempest run --config-file /etc/tempest/tempest.conf --blacklist-file /tempest/blacklist --subunit &gt; /tmp/results.subunit&apos;docker exec tempest bash -c &apos;subunit2html /tmp/results.subunit /tmp/results.html&apos;docker cp tempest:/tmp/results.html $TEMPEST_FIL","categories":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/child/categories/openstack/"}],"tags":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/child/tags/openstack/"},{"name":"tempest","slug":"tempest","permalink":"http://yoursite.com/child/tags/tempest/"}]},{"title":"opneshift 多节点测试","slug":"openshift/openshift-02","date":"2017-08-01T04:02:00.000Z","updated":"2017-09-22T02:21:00.000Z","comments":true,"path":"2017/08/01/openshift/openshift-02/","link":"","permalink":"http://yoursite.com/child/2017/08/01/openshift/openshift-02/","excerpt":"","text":"Openshift 多节点测试部署设置inventory1234567891011121314151617181920212223242526272829303132333435363738vim mul-inventory[OSEv3:children]mastersnodesetcd[OSEv3:vars]ansible_ssh_user=rootopenshift_deployment_type=originopenshif_release=v1.5openshift_master_identity_providers=[&#123;&apos;name&apos;: &apos;keystone_provider&apos;, &apos;login&apos;: &apos;true&apos;, &apos;challenge&apos;: &apos;true&apos;, &apos;kind&apos;: &apos;KeystonePasswordIdentityProvider&apos;, &apos;domainName&apos;: &apos;default&apos;, &apos;url&apos;: &apos;http://172.16.130.214:5000&apos; &#125;]openshift_docker_options=&quot;-l warn --ipv6=false&quot;openshift_master_cluster_method=nativeopenshift_master_cluster_hostname=node1.99cloud.comopenshift_master_cluster_public_hostname=172.16.50.43openshift_additional_repos=[&#123;&apos;id&apos;: &apos;openshift-origin-local&apos;, &apos;name&apos;: &apos;OpenShift Origin&apos;, &apos;baseurl&apos;: &apos;http://172.16.50.43:8040/centos-openshift-origin&apos;, &apos;enabled&apos;: 1, &apos;gpgcheck&apos;: 0&#125;,&#123;&apos;id&apos;: &apos;openshift-bse-local&apos;, &apos;name&apos;: &apos;OpenShift Base&apos;, &apos;baseurl&apos;: &apos;http://172.16.50.43:8040/base&apos;, &apos;enabled&apos;: 1, &apos;gpgcheck&apos;: 0&#125;]openshift_docker_additional_registries=172.16.50.43:5000openshift_docker_insecure_registries=0.0.0.0/0openshift_examples_modify_imagestreams=trueopenshift_cockpit_deployer_prefix=172.16.50.43:5000/openshift/openshift_cockpit=172.16.50.43:5000/openshift/cockpit/kubernetesopenshift_disable_check=memory_availability,disk_availability,package_availability,package_update,docker_image_availability,docker_storage_driver,docker_storage[masters]openshif-2[etcd]openshif-2openshif-3openshif-4[nodes]openshif-3 openshift_schedulable=true openshift_node_labels=&quot;&#123;&apos;region&apos;: &apos;infra&apos;, &apos;zone&apos;: &apos;default&apos;&#125;&quot;openshif-2 openshift_schedulable=trueopenshif-4 openshift_schedulable=true 执行部署命令 123456[root@openshif-2 ~]# ansible-playbook ~/zhu/openshift-ansible/playbooks/byo/config.yml -i mul-inventory[root@openshif-2 ~]# kubectl get nodeNAME STATUS AGE172.16.50.33 Ready 16h172.16.50.37 Ready 16h172.16.50.54 Ready 16h 部署Openshift ha多节点测试默认master节点负载均衡是haproxy，编写inventory 1234567891011121314151617181920212223242526272829303132333435363738vim ha-inventory[OSEv3:children]mastersnodesetcdlb[OSEv3:vars]ansible_ssh_user=rootopenshift_deployment_type=originopenshif_release=v1.5openshift_master_identity_providers=[&#123;&apos;name&apos;: &apos;keystone_provider&apos;, &apos;login&apos;: &apos;true&apos;, &apos;challenge&apos;: &apos;true&apos;, &apos;kind&apos;: &apos;KeystonePasswordIdentityProvider&apos;, &apos;domainName&apos;: &apos;default&apos;, &apos;url&apos;: &apos;http://172.16.130.214:5000&apos; &#125;]openshift_docker_options=&quot;-l warn --ipv6=false&quot;### ha要打开native这个选项openshift_master_cluster_method=nativeopenshift_master_cluster_hostname=node1.99cloud.comopenshift_master_cluster_public_hostname=172.16.50.43openshift_additional_repos=[&#123;&apos;id&apos;: &apos;openshift-origin-local&apos;, &apos;name&apos;: &apos;OpenShift Origin&apos;, &apos;baseurl&apos;: &apos;http://172.16.50.43:8040/centos-openshift-origin&apos;, &apos;enabled&apos;: 1, &apos;gpgcheck&apos;: 0&#125;,&#123;&apos;id&apos;: &apos;openshift-bse-local&apos;, &apos;name&apos;: &apos;OpenShift Base&apos;, &apos;baseurl&apos;: &apos;http://172.16.50.43:8040/base&apos;, &apos;enabled&apos;: 1, &apos;gpgcheck&apos;: 0&#125;]openshift_docker_additional_registries=172.16.50.43:5000openshift_docker_insecure_registries=0.0.0.0/0openshift_examples_modify_imagestreams=trueopenshift_cockpit_deployer_prefix=172.16.50.43:5000/openshift/openshift_cockpit=172.16.50.43:5000/openshift/cockpit/kubernetesopenshift_disable_check=memory_availability,disk_availability,package_availability,package_update,docker_image_availability,docker_storage_driver,docker_storage[etcd]openshif-2openshif-3openshif-4[lb]openshif-1[nodes]openshif-3 openshift_schedulable=true openshift_node_labels=&quot;&#123;&apos;region&apos;: &apos;infra&apos;, &apos;zone&apos;: &apos;default&apos;&#125;&quot;openshif-2 openshift_schedulable=trueopenshif-4 openshift_schedulable=true 执行命令 123456[root@openshif-2 ~]# ansible-playbook ~/zhu/openshift-ansible/playbooks/byo/config.yml -i mul-inventory[root@openshif-2 ~]# kubectl get nodeNAME STATUS AGE172.16.50.33 Ready 16h172.16.50.37 Ready 16h172.16.50.54 Ready 16h 参考文档：https://docs.openshift.org/latest/install_config/install/advanced_install.html","categories":[{"name":"openshift","slug":"openshift","permalink":"http://yoursite.com/child/categories/openshift/"}],"tags":[{"name":"openshift","slug":"openshift","permalink":"http://yoursite.com/child/tags/openshift/"},{"name":"pass","slug":"pass","permalink":"http://yoursite.com/child/tags/pass/"}]},{"title":"kolla 介绍","slug":"openstack/kolla-01","date":"2017-07-23T22:30:00.000Z","updated":"2017-09-22T02:21:03.000Z","comments":true,"path":"2017/07/24/openstack/kolla-01/","link":"","permalink":"http://yoursite.com/child/2017/07/24/openstack/kolla-01/","excerpt":"","text":"kolla总结在kolla项目下也快1年啦，也该总结下, 文笔不好权当记录。 kolla的定位实现生产级别容器化openstack平台，做到开箱即用 kolla的实现kolla实现主要以docker和ansible实现的。ansible实现编排和远程推送docker实现openstack服务容器化 kolla的实现带来什么变化1.让安装变得更加简单 2.环境一致性 3.让openstack升级不是困难 4.灵活性大大加强 5.openstack服务微服务化 kolla代码目录 从顶层文件目录结构来看，各个目录所包含的内容为： ansible ansible 配置目录里面包含了openstack service ansible role的实现， Kolla - Kolla with ansible!，是kolla-ansible最为重要的一部分。 contrib 示例目录。里面包含了 heat 的编排配置和magnum的一些例子 ； docs 文档目录。也是非常重要的目录，里面包括开发环境设置、镜像编译、Kolla 环境变量等说明，建议把文档都认真读一遍 ； LICENSE LICENSE文件。Apache License Version 2 的 License 文件，没什么好说的。 README.md 说明文件。 specs spec目录。目前只有一个 spec，说明使用容器安装 OpenStack 的理念和优势。 test-requirements.txt python 的 requirements 文件。用于说明测试时所需要的 python 包，目前只有一个 PyYAML。 tests 测试目录。这个目录应该包含 Kolla 的测试套件，但目前只有一个 setup_docker.sh 用于安装 docker。 tools 工具脚本目录。目录包含编译 docker 镜像、清理 docker 环境、生成 Kolla 环境变量、Kolla 启动脚本、json/yaml文件检验等脚本。建议把这个目录的脚本都看一遍，需要点 Shell、python的知识。 tox.ini tox配置文件。tox是一个标准自动化测试工具，python里的。目前这个文件很简单，只包含了 virtualenv 设置和一些简单的检查。 现在 Kolla 还小，不像 nova / neutron 这些庞然大物，因此很值得把 Kolla 的代码认真看看，整理 Kolla 的设计思路，对以后把握 Kolla 的发展很有帮助，也希望大家能参与到 Kolla 的社区中，无论是贡献代码、写写 Blog、找茬，都能帮助到 Kolla 关于kolla方面的文章沙克写的很好，大家可以看看。这个他的博客链接：http://www.chenshake.com/cloud-computing/","categories":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/child/categories/openstack/"}],"tags":[{"name":"kolla","slug":"kolla","permalink":"http://yoursite.com/child/tags/kolla/"},{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/child/tags/openstack/"}]},{"title":"openshift部署之前环境准备","slug":"openshift/openshift-env-reday","date":"2017-07-16T22:30:00.000Z","updated":"2017-09-22T02:01:08.000Z","comments":true,"path":"2017/07/17/openshift/openshift-env-reday/","link":"","permalink":"http://yoursite.com/child/2017/07/17/openshift/openshift-env-reday/","excerpt":"","text":"openshift部署之前环境准备准备工作(Prerequisites)Install stand-alone registry To install OpenShift Origin as a stand-alone registry, see Installing a Stand-alone Registry. System Requirements Master 1. base os: Fedora 21, CentOS 7.3, or RHEL 7.3 with the &quot;Minimal&quot; installation option and the latest packages from the Extras channel, or RHEL Atomic Host 7.3.2 or later. RHEL 7.2 is also supported using Docker 1.12 and its dependencies. 2. 2vcpu 3. Minimum 16 GB RAM 4. Minimum 40 GB hard disk space for the file system containing /var/. Nodes 1. Physical or virtual system, or an instance running on a public or private IaaS. 2. Base OS: Fedora 21, CentOS 7.3, or RHEL 7.3 or later with &quot;Minimal&quot; installation option, or RHEL Atomic Host 7.3.2 or later. RHEL 7.2 is also supported using Docker 1.12 and its dependencies. 3. NetworkManager 1.0 or later. 4. 1 vCPU. 5. Minimum 8 GB RAM. 6. Minimum 15 GB hard disk space for the file system containing /var/. 7. An additional minimum 15 GB unallocated space to be used for Docker’s storage back end; see Configuring Docker Storage. External etcd Nodes 1. Minimum 20 GB hard disk space for etcd data. 2. Consult Hardware Recommendations to properly size your etcd nodes. 3. Currently, OpenShift Origin stores image, build, and deployment metadata in etcd. You must periodically prune old resources. If you are planning to leverage a large number of images/builds/deployments, place etcd on machines with large amounts of memory and fast SSD drives. openshift设置缓存openshift master 主机节点高速缓存目的是为了缓解CPU负载。然而，在小于1000pod的较小集群中，该缓存可能会浪费大量内存，从而忽略CPU负载的降低。 默认缓存大小为50000个条目，根据资源大小，可以增加到1到2Gb的内存。可以使用／etc/origin/master/master-config.yaml中的以下设置来减少此缓存大小。 1234kubernetesMasterConfig: apiServerArguments: deserialization-cache-size: - &quot;1000&quot; 设置core使用默认情况下，openshift origin master和node节点使用他们运行系统中的所有内核，你可以通过设置GOMAXPROCS环境变量来选择 我们所希望openshfit使用的内核数；12# openshift使用1个内核export GOMAXPROCS=1 SELinuxSecurity-Enhanced Linux (SELinux)在安装openshit之前必须在所以节点打开（这点好怪，其他的例如openstack要关闭）不然会安装失败。在 /etc/selinux/config 文件中配置SELINUXTYPE=targeted 1234567891011# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.SELINUX=enforcing# SELINUXTYPE= can take one of these three values:# targeted - Targeted processes are protected,# minimum - Modification of targeted policy. Only selected processes are protected.# mls - Multi Level Security protection.SELINUXTYPE=targeted NTP时间同步不用多说，设置openshift_clock_enabled to true打开1openshift_clock_enabled to true DNSOpenShift Origin在环境中需要一个功能齐全的DNS服务器。 这是理想的运行DNS软件的独立主机，可以为在平台上运行的主机和容器提供名称解析。 默认情况下，容器从主机收到dns配置文件进行解析(/etc/reslov.conf) contaier dns openshift origin 将dns值插入到pods（节点的命名空间里面），这个值在/etc/orgin/node/node-config.yaml文件中由dnsIP参数定义，dnsIP参数默认设置为主机节点的地址，因为主机使用dnsmasq. 如果在node-config.yaml文件中忽略dnpIP这个参数，就会默认为kubernetes service ip， 它是该pod的/etc/resolv.conf文件中的第一个名称服务器。 从OpenShift origin 1.2开始， dnsmasq自动配置到所以主节点（master）和节点（node）上。pod使用节点作为它的dns，然后节点接受所有转发请求。默认情况下，dnsmasq在节点上进行配置监听 53端口， 所以节点不能运行其他的类型的dns应用程序。 节点上需要NetworkManager才能使用DNS IP地址填充dnsmasq. 下面是一个例子 123master A 10.64.33.100node1 A 10.64.33.101node2 A 10.64.33.102 如果你没有正常运行dns缓解，你下面的操作可能会失败： 通过ansible脚本去安装产品 基础设施容器的部署（registry， routers） 访问OpenShif Origin Web 控制台， 因为它不能通过ip地址单独访问。 配置hosts使用dns 确保您的环境中的每个主机都配置为从DNS服务器解析主机名。主机DNS解析的配置取决于是否启用DHCP。 如果dhcp服务器是禁用的， 应该将您的网络接口配置为静态，并将DNS名称服务器添加到NetworkManager。 如果dhcp服务器是启用的，NetworkManager调度脚本将根据DHCP配置自动配置DNS。或者，您可以在node-config.yaml文件中的dnsIP中添加一个值，以将pod的resolv.conf文件添加到前面。然后，第二个名称服务器由主机的第一个名称服务器定义。默认情况下，这将是节点主机的IP地址。 对于大多数配置，请勿在OpenShift Origin（使用Ansible）高级安装期间设置openshift_dns_ip选项，因为此选项将覆盖由dnsIP设置的默认IP地址。 相反，允许安装程序配置每个节点使用dnsmasq并将请求转发给SkyDNS或外部DNS提供程序。如果您设置了openshift_dns_ip选项，则应该使用首先查询SkyDNS的DNS IP或SkyDNS服务或端点IP（Kubernetes服务IP）进行设置。 验证dns 服务器能够解析hosts 检查/etc/resolv.conf内容 12345$ cat /etc/resolv.conf# Generated by NetworkManagersearch example.comnameserver 10.64.33.1# nameserver updated by /etc/NetworkManager/dispatcher.d/99-origin-dns.sh 在这个例子中， 10.64.33.1 就是我们的DNS server. 2.测试/etc/resolv.conf中列出的DNS服务器是否可以将主机名解析为OpenShift Origin环境中所有主节点和IP节点的IP地址 123456# dig &lt;node_hostname&gt; @&lt;IP_address&gt; +short$ dig master.example.com @10.64.33.1 +short10.64.33.100$ dig node1.example.com @10.64.33.1 +short10.64.33.101","categories":[{"name":"openshift","slug":"openshift","permalink":"http://yoursite.com/child/categories/openshift/"}],"tags":[{"name":"openshift","slug":"openshift","permalink":"http://yoursite.com/child/tags/openshift/"},{"name":"pass","slug":"pass","permalink":"http://yoursite.com/child/tags/pass/"}]},{"title":"rally-01","slug":"openstack/rally","date":"2017-07-16T04:30:00.000Z","updated":"2017-09-22T02:21:09.000Z","comments":true,"path":"2017/07/16/openstack/rally/","link":"","permalink":"http://yoursite.com/child/2017/07/16/openstack/rally/","excerpt":"","text":"openstack 测试介绍Rally 介绍介绍贴图：文字介绍 使用kolla部署rallyrally的功能介绍rally的基本使用rally调用tempestrally的局限性OpenStack，毫无疑问是各种服务联合的一个庞大的生态系统。 Rally 作为一个标准测试工具，回答了 “OpenStack 如何大规模运作？” 的问题。 为了使其成为可能，Rally 自动化并统一了多个节点的 OpenStack 部署、云的验证、标准测试和分析。Rally 使用一个通用的方法，使得我们能够检查 OpenStack 是否正常运作，也即，在高负荷下 1K 服务器的安装是否成功。因此，它可以作为 OpenStack CI/CD 系统的基本工具来使用，可以持续改进其 SLA、运作和稳定性。 Rally 的操作如下图所示：http://coffeechou.github.io/public/imgs/rally_actions.png 在kolla中使用rally12345678910#在配置文件打开rallyvim /etc/kolla/globals.ymlenable_rally: &quot;yes&quot;#部署环境kolla-ansible deploy#生成openrc环境变量kolla-ansible post-deploy 初始化rallyRegistering an OpenStack deployment in Rally123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129# 进入rally 容器[root@control01 ~]# docker exec -it -u root rally bash(rally)[root@control01 /]#(rally)[root@control01 deployments]# vim existing-keystone-v3.json&#123; &quot;type&quot;: &quot;ExistingCloud&quot;, &quot;auth_url&quot;: &quot;http://172.16.130.210:5000/v3&quot;, &quot;region_name&quot;: &quot;RegionOne&quot;, &quot;endpoint_type&quot;: &quot;public&quot;, &quot;admin&quot;: &#123; &quot;username&quot;: &quot;admin&quot;, &quot;password&quot;: &quot;99cloud&quot;, &quot;user_domain_name&quot;: &quot;deault&quot;, &quot;project_name&quot;: &quot;admin&quot;, &quot;project_domain_name&quot;: &quot;deault&quot; &#125;, &quot;https_insecure&quot;: false, &quot;https_cacert&quot;: &quot;&quot;&#125;&#123;% qnimg test/test.png title:test alt:test %&#125;### 创建deployments(rally)[root@control01 deployments]#rally deployment create --file existing-keystone-v3.json --name 99cloud-regionOne+--------------------------------------+---------------------+-------------------+------------------+--------+| uuid | created_at | name | status | active |+--------------------------------------+---------------------+-------------------+------------------+--------+| e36d802c-2211-4bac-a8ea-2f3b967568b9 | 2017-05-08 03:07:46 | 99cloud-regionOne | deploy-&gt;finished | |+--------------------------------------+---------------------+-------------------+------------------+--------+Using deployment: e36d802c-2211-4bac-a8ea-2f3b967568b9~/.rally/openrc was updatedHINTS:* To use standard OpenStack clients, set up your env by running: source ~/.rally/openrc OpenStack clients are now configured, e.g run: openstack image list(rally)[root@control01 deployments]# rally deployment list+--------------------------------------+---------------------+-------------------+-------------------+--------+| uuid | created_at | name | status | active |+--------------------------------------+---------------------+-------------------+-------------------+--------+| af993ae9-363e-4701-b343-626063eaf715 | 2017-04-26 07:13:50 | existing | cleanup-&gt;finished | || e36d802c-2211-4bac-a8ea-2f3b967568b9 | 2017-05-08 03:07:46 | 99cloud-regionOne | deploy-&gt;finished | * |+--------------------------------------+---------------------+-------------------+-------------------+--### 还有一种创建方法，就是使用环境变量去创建### 我们先删除之前创建的(rally)[root@control01 deployments]# rally deployment destroy 99cloud-regionOne(rally)[root@control01 deployments]# rally deployment list+--------------------------------------+---------------------+----------+-------------------+--------+| uuid | created_at | name | status | active |+--------------------------------------+---------------------+----------+-------------------+--------+| af993ae9-363e-4701-b343-626063eaf715 | 2017-04-26 07:13:50 | existing | cleanup-&gt;finished | |+--------------------------------------+---------------------+----------+-------------------+--------+### (rally)[root@control01 /]# cat openrcexport OS_REGION_NAME=RegionOneexport OS_PROJECT_DOMAIN_NAME=defaultexport OS_USER_DOMAIN_NAME=defaultexport OS_PROJECT_NAME=adminexport OS_TENANT_NAME=adminexport OS_USERNAME=adminexport OS_PASSWORD=99cloudexport OS_AUTH_URL=http://172.16.130.210:35357/v3export OS_INTERFACE=internalexport OS_IDENTITY_API_VERSION=3(rally)[root@control01 /]# source openrc## 测试下环境变量是否可用(rally)[root@control01 deployments]# openstack image list+--------------------------------------+-----------------------------------+--------+| ID | Name | Status |+--------------------------------------+-----------------------------------+--------+| 83dc79c8-c6df-4e23-808c-0209778be1b2 | 123 | active || 9d4cc662-bdfb-46cb-996d-3b773ee161d5 | a1 | active || 255cc63c-f635-4bbb-850c-e319d2e4c17d | cenos-raw | active || 63c5d475-e96a-4c1f-985b-12685aa2a95b | centos6.5 | active || 15ed4618-4b3b-4cc3-a9ef-6967e3a3091d | centos7 | active || 15f04241-38cc-482d-b6a3-42ba9d29f47a | cirros | active || 6491747b-2fea-4d7c-b034-b3fe5089df2c | d1 | active || 4f99d882-04af-47bc-9e2d-a12aaecbf753 | s_rally_e4dbf0c1_LdpjrVlZ-shelved | active || 6a45e590-08fc-471f-b4d8-9a6195e88d07 | s_rally_e4dbf0c1_f3TP5nP4-shelved | active || a166585b-fb88-4c0a-a0c9-47443e9ca8be | s_rally_e4dbf0c1_lEthjCi9-shelved | active || fa680415-4448-4564-9b22-bdf2b86b6cd2 | s_rally_e4dbf0c1_nuqTKzuQ-shelved | active || 6eec3597-ee52-436c-9970-ec11415cbb4e | sdf1 | active || 1e84937a-a700-447b-8ffc-3f162127300d | snap1 | active |+--------------------------------------+-----------------------------------+--------+(rally)[root@control01 deployments]# rally deployment create --fromenv --name regiOne-99cloud+--------------------------------------+---------------------+-----------------+------------------+--------+| uuid | created_at | name | status | active |+--------------------------------------+---------------------+-----------------+------------------+--------+| 6682d272-5d99-400b-9de9-c4d46a705ba6 | 2017-05-08 03:21:46 | regiOne-99cloud | deploy-&gt;finished | |+--------------------------------------+---------------------+-----------------+------------------+--------+Using deployment: 6682d272-5d99-400b-9de9-c4d46a705ba6~/.rally/openrc was updatedHINTS:* To use standard OpenStack clients, set up your env by running: source ~/.rally/openrc OpenStack clients are now configured, e.g run: openstack image list(rally)[root@control01 deployments]# source ~/.rally/openrc### 测试deployments是否可用(rally)[root@control01 deployments]# openstack image list+--------------------------------------+-----------------------------------+--------+| ID | Name | Status |+--------------------------------------+-----------------------------------+--------+| 83dc79c8-c6df-4e23-808c-0209778be1b2 | 123 | active || 9d4cc662-bdfb-46cb-996d-3b773ee161d5 | a1 | active || 255cc63c-f635-4bbb-850c-e319d2e4c17d | cenos-raw | active || 63c5d475-e96a-4c1f-985b-12685aa2a95b | centos6.5 | active || 15ed4618-4b3b-4cc3-a9ef-6967e3a3091d | centos7 | active || 15f04241-38cc-482d-b6a3-42ba9d29f47a | cirros | active || 6491747b-2fea-4d7c-b034-b3fe5089df2c | d1 | active || 4f99d882-04af-47bc-9e2d-a12aaecbf753 | s_rally_e4dbf0c1_LdpjrVlZ-shelved | active || 6a45e590-08fc-471f-b4d8-9a6195e88d07 | s_rally_e4dbf0c1_f3TP5nP4-shelved | active || a166585b-fb88-4c0a-a0c9-47443e9ca8be | s_rally_e4dbf0c1_lEthjCi9-shelved | active || fa680415-4448-4564-9b22-bdf2b86b6cd2 | s_rally_e4dbf0c1_nuqTKzuQ-shelved | active || 6eec3597-ee52-436c-9970-ec11415cbb4e | sdf1 | active || 1e84937a-a700-447b-8ffc-3f162127300d | snap1 | active |+--------------------------------------+-----------------------------------+--------+(rally)[root@control01 deployments]# rally deployment list+--------------------------------------+---------------------+-----------------+-------------------+--------+| uuid | created_at | name | status | active |+--------------------------------------+---------------------+-----------------+-------------------+--------+| af993ae9-363e-4701-b343-626063eaf715 | 2017-04-26 07:13:50 | existing | cleanup-&gt;finished | || 6682d272-5d99-400b-9de9-c4d46a705ba6 | 2017-05-08 03:21:46 | regiOne-99cloud | deploy-&gt;finished | * |+--------------------------------------+---------------------+-----------------+-------------------+--------+","categories":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/child/categories/openstack/"}],"tags":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/child/tags/openstack/"},{"name":"rally","slug":"rally","permalink":"http://yoursite.com/child/tags/rally/"}]},{"title":"openshift iso 制作","slug":"openshift/openshift-iso","date":"2017-07-14T04:37:00.000Z","updated":"2017-09-22T02:20:57.000Z","comments":true,"path":"2017/07/14/openshift/openshift-iso/","link":"","permalink":"http://yoursite.com/child/2017/07/14/openshift/openshift-iso/","excerpt":"","text":"制作iso过程文档制作openshift isoFedora发行版制作的工具主要有revisor和pungi两种；revisor是一个图形化的工具，也可用命令行，但是经常会有bug，而且比较臃肿，感觉是给初级用户用的；pungi是Fedora官方制作（或叫spin）发行版的工具，命令行，总共也就4、5个Python文件；下面是使用 pungi来制作的iso 安装pungi以下都是环境都是基于centos7 123456789## 安装epel repoyum install -y epel-release## 安装elreporpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.orgrpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm## 安装 pungiyum install -y pungi hfsplus-tools kmod-hfs kmod-hfsplus docker 使用pungi build iso1234567891011121314151617[root@localhost kolla-iso]# git clone http://gitlab.sh.99cloud.net/zhubingbing/openshift-iso.git[root@localhost kolla-iso]# cd openshif-iso[root@localhost kolla-iso]# lsbuild.sh cobbler grub.cfg isolinux.cfg ks.cfg openshift os_ks.cfg README.md## 这些包自己预先准备好，因为过大就没放在github上面，路径是存在 openshif-iso/openshift/目录下面，如果没有这个目录自己建立[root@localhost kolla-iso]# ls openshift/centos-openshift-origin.tar docker-registry.tar openshift-ansible.tar.gz openshift-package.tar registry.tar[root@localhost openshift]# du -h openshift/1.8G .[root@localhost kolla-iso]# ./build.sh cobblerSending build context to Docker daemon 34.43 MBStep 1 : FROM centos ---&gt; 36540f359ca3Step 2 : RUN yum install -y epel-release[root@localhost kolla-iso] ./build //等待10几分钟iso就build好了ls /root/kolla-iso/Ocata/x86_64/iso/CentOS-DVD-x86_64-Ocata.iso build.sh代码12345678910111213141516171819202122232425262728#!/bin/bash -ecase $1 in &apos;&apos;)pungi --name=CentOS --ver=Ocata -c ks.cfg --isfinal --nosource --nodebuginfo --force -G -C -Brsync -a isolinux.cfg Ocata/x86_64/os/isolinux/rsync -a os_ks.cfg Ocata/x86_64/os/images/ks.cfgmount Ocata/x86_64/os/images/efiboot.img /mntrsync -a grub.cfg /mnt/EFI/BOOT/rsync -a grub.cfg Ocata/x86_64/os/EFI/BOOT/umount /mntrm -f Ocata/x86_64/os/images/macboot.imgmkdir Ocata/x86_64/os/extrasrsync -a cobbler/init.sh Ocata/x86_64/os/extrasrsync -a cobbler/cobbler.tar Ocata/x86_64/os/extrasrsync -a openshift/registry.tar Ocata/x86_64/os/extrasrsync -a openshift/docker-registry.tar Ocata/x86_64/os/extrasrsync -a openshift/openshift-ansible.tar.gz Ocata/x86_64/os/extrasrsync -a openshift/openshift-package.tar Ocata/x86_64/os/extraspungi --name=CentOS --ver=Ocata -c ks.cfg --isfinal --nosource --nodebuginfo --force -I ;; &quot;cobbler&quot; )docker build -t cobbler cobbler/build/; docker save cobbler &gt; cobbler/cobbler.tar ;; &quot;clean&quot;)rm -rf centos centos-extras docker epel logs ourtree work ocata Ocata cobbler/cobbler.tar ;;esac 测试ISO使用openshift iso安装部署节点设置virtualbox虚拟机从光盘启动虚拟机配置：2c 4g 40g 使用iso安装openshit部署节点使用上下键选择安装openshift， 按下table键设置ip地址，确认（enter）开始安装 安装完成确认3件事 registry 是否正常启动 cobbler 是否正常 openshift yum源是否正常 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192### cobbler 容器和registry容器正常启动[root@control01 ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe01bced0a610 registry:2 &quot;/entrypoint.sh /etc/&quot; 28 seconds ago Up 26 seconds 0.0.0.0:4000-&gt;5000/tcp registryb651bdf9c0c2 cobbler &quot;/usr/bin/supervisord&quot; 2 minutes ago Up 2 minutes cobbler### 检查registry里面镜像是否正常[root@control01 openshift]# pwd/registry/docker/registry/v2/repositories/openshift[root@control01 openshift]# lscockpit origin-deployer origin-docker-registry origin-haproxy-router origin-metrics-cassandra origin-metrics-hawkular-metrics origin-metrics-heapster origin-pod### 检查openshift yum源是否可以用[root@control01 openshift]# curl http://192.168.56.100:81/openshift-package/&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 3.2 Final//EN&quot;&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;Index of /openshift-package&lt;/title&gt; &lt;/head&gt; &lt;body&gt;&lt;h1&gt;Index of /openshift-package&lt;/h1&gt; &lt;table&gt; &lt;tr&gt;&lt;th valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/blank.gif&quot; alt=&quot;[ICO]&quot;&gt;&lt;/th&gt;&lt;th&gt;&lt;a href=&quot;?C=N;O=D&quot;&gt;Name&lt;/a&gt;&lt;/th&gt;&lt;th&gt;&lt;a href=&quot;?C=M;O=A&quot;&gt;Last modified&lt;/a&gt;&lt;/th&gt;&lt;th&gt;&lt;a href=&quot;?C=S;O=A&quot;&gt;Size&lt;/a&gt;&lt;/th&gt;&lt;th&gt;&lt;a href=&quot;?C=D;O=A&quot;&gt;Description&lt;/a&gt;&lt;/th&gt;&lt;/tr&gt; &lt;tr&gt;&lt;th colspan=&quot;5&quot;&gt;&lt;hr&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/back.gif&quot; alt=&quot;[PARENTDIR]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;/&quot;&gt;Parent Directory&lt;/a&gt; &lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt; - &lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;ansible-2.3.1.0-1.el7.noarch.rpm&quot;&gt;ansible-2.3.1.0-1.el..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 14:19 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;5.7M&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;at-3.1.13-22.el7.x86_64.rpm&quot;&gt;at-3.1.13-22.el7.x86..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 51K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;attr-2.4.46-12.el7.x86_64.rpm&quot;&gt;attr-2.4.46-12.el7.x..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 66K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;avahi-libs-0.6.31-17.el7.x86_64.rpm&quot;&gt;avahi-libs-0.6.31-17..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 61K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;bash-completion-2.1-6.el7.noarch.rpm&quot;&gt;bash-completion-2.1-..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 14:19 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 85K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;bc-1.06.95-13.el7.x86_64.rpm&quot;&gt;bc-1.06.95-13.el7.x8..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;115K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;bridge-utils-1.5-9.el7.x86_64.rpm&quot;&gt;bridge-utils-1.5-9.e..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 32K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;ceph-common-0.94.5-1.el7.x86_64.rpm&quot;&gt;ceph-common-0.94.5-1..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;6.2M&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;conntrack-tools-1.4.4-3.el7_3.x86_64.rpm&quot;&gt;conntrack-tools-1.4...&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;186K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;cryptsetup-1.7.2-1.el7.x86_64.rpm&quot;&gt;cryptsetup-1.7.2-1.e..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;124K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;cups-client-1.6.3-26.el7.x86_64.rpm&quot;&gt;cups-client-1.6.3-26..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;149K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;cups-libs-1.6.3-26.el7.x86_64.rpm&quot;&gt;cups-libs-1.6.3-26.e..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;356K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;ed-1.9-4.el7.x86_64.rpm&quot;&gt;ed-1.9-4.el7.x86_64.rpm&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 72K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;epel-release-7-9.noarch.rpm&quot;&gt;epel-release-7-9.noa..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 14:19 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 14K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;etcd-3.1.9-1.el7.x86_64.rpm&quot;&gt;etcd-3.1.9-1.el7.x86..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 14:19 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;7.3M&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;gdisk-0.8.6-5.el7.x86_64.rpm&quot;&gt;gdisk-0.8.6-5.el7.x8..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;187K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;glusterfs-fuse-3.7.9-12.el7.centos.x86_64.rpm&quot;&gt;glusterfs-fuse-3.7.9..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;109K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;gssproxy-0.4.1-13.el7.x86_64.rpm&quot;&gt;gssproxy-0.4.1-13.el..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 87K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;iptables-services-1.4.21-17.el7.x86_64.rpm&quot;&gt;iptables-services-1...&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 14:19 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 50K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;iscsi-initiator-utils-6.2.0.873-35.el7.x86_64.rpm&quot;&gt;iscsi-initiator-util..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;417K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;iscsi-initiator-utils-iscsiuio-6.2.0.873-35.el7.x86_64.rpm&quot;&gt;iscsi-initiator-util..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 85K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;keyutils-1.5.8-3.el7.x86_64.rpm&quot;&gt;keyutils-1.5.8-3.el7..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 54K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;libbasicobjects-0.1.1-27.el7.x86_64.rpm&quot;&gt;libbasicobjects-0.1...&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 25K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;libcollection-0.6.2-27.el7.x86_64.rpm&quot;&gt;libcollection-0.6.2-..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 41K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;libicu-50.1.2-15.el7.x86_64.rpm&quot;&gt;libicu-50.1.2-15.el7..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;6.9M&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;libini_config-1.3.0-27.el7.x86_64.rpm&quot;&gt;libini_config-1.3.0-..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 63K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;libnetfilter_cthelper-1.0.0-9.el7.x86_64.rpm&quot;&gt;libnetfilter_cthelpe..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 18K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;libnetfilter_cttimeout-1.0.0-6.el7.x86_64.rpm&quot;&gt;libnetfilter_cttimeo..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 18K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;libnetfilter_queue-1.0.2-2.el7.x86_64.rpm&quot;&gt;libnetfilter_queue-1..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 23K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;libnfsidmap-0.25-15.el7.x86_64.rpm&quot;&gt;libnfsidmap-0.25-15...&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 47K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;libpath_utils-0.2.1-27.el7.x86_64.rpm&quot;&gt;libpath_utils-0.2.1-..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 27K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;libref_array-0.1.5-27.el7.x86_64.rpm&quot;&gt;libref_array-0.1.5-2..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 26K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;libtalloc-2.1.6-1.el7.x86_64.rpm&quot;&gt;libtalloc-2.1.6-1.el..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 34K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;libtevent-0.9.28-1.el7.x86_64.rpm&quot;&gt;libtevent-0.9.28-1.e..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 34K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;libtirpc-0.2.4-0.8.el7_3.x86_64.rpm&quot;&gt;libtirpc-0.2.4-0.8.e..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 88K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;libverto-tevent-0.2.5-4.el7.x86_64.rpm&quot;&gt;libverto-tevent-0.2...&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;9.0K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;m4-1.4.16-10.el7.x86_64.rpm&quot;&gt;m4-1.4.16-10.el7.x86..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;256K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;mailx-12.5-12.el7_0.x86_64.rpm&quot;&gt;mailx-12.5-12.el7_0...&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;244K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;nfs-utils-1.3.0-0.33.el7_3.x86_64.rpm&quot;&gt;nfs-utils-1.3.0-0.33..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;377K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;openvswitch-2.6.1-3.git20161206.el7.x86_64.rpm&quot;&gt;openvswitch-2.6.1-3...&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;4.9M&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;origin-1.5.1-1.el7.x86_64.rpm&quot;&gt;origin-1.5.1-1.el7.x..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 14:19 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 35M&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;origin-clients-1.5.1-1.el7.x86_64.rpm&quot;&gt;origin-clients-1.5.1..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 14:19 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 16M&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;origin-docker-excluder-1.5.1-1.el7.noarch.rpm&quot;&gt;origin-docker-exclud..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 14:19 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;6.1K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;origin-excluder-1.5.1-1.el7.noarch.rpm&quot;&gt;origin-excluder-1.5...&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 14:19 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;6.1K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;origin-master-1.5.1-1.el7.x86_64.rpm&quot;&gt;origin-master-1.5.1-..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 14:19 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;9.2K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;origin-node-1.5.1-1.el7.x86_64.rpm&quot;&gt;origin-node-1.5.1-1...&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;7.0K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;origin-sdn-ovs-1.5.1-1.el7.x86_64.rpm&quot;&gt;origin-sdn-ovs-1.5.1..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;3.1M&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;patch-2.7.1-8.el7.x86_64.rpm&quot;&gt;patch-2.7.1-8.el7.x8..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;110K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;psmisc-22.20-11.el7.x86_64.rpm&quot;&gt;psmisc-22.20-11.el7...&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;141K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;python-keyczar-0.71c-2.el7.noarch.rpm&quot;&gt;python-keyczar-0.71c..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 14:19 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;218K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;python-rados-0.94.5-1.el7.x86_64.rpm&quot;&gt;python-rados-0.94.5-..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 38K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;python-rbd-0.94.5-1.el7.x86_64.rpm&quot;&gt;python-rbd-0.94.5-1...&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 28K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;python2-ecdsa-0.13-4.el7.noarch.rpm&quot;&gt;python2-ecdsa-0.13-4..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 14:19 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 83K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;python2-paramiko-1.16.1-2.el7.noarch.rpm&quot;&gt;python2-paramiko-1.1..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 14:19 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;258K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;python2-pyasn1-0.1.9-7.el7.noarch.rpm&quot;&gt;python2-pyasn1-0.1.9..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 14:19 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;100K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;quota-4.01-14.el7.x86_64.rpm&quot;&gt;quota-4.01-14.el7.x8..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;179K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;quota-nls-4.01-14.el7.noarch.rpm&quot;&gt;quota-nls-4.01-14.el..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 90K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;redhat-lsb-core-4.1-27.el7.centos.1.x86_64.rpm&quot;&gt;redhat-lsb-core-4.1-..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 38K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;redhat-lsb-submod-security-4.1-27.el7.centos.1.x86_64.rpm&quot;&gt;redhat-lsb-submod-se..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 15K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/folder.gif&quot; alt=&quot;[DIR]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;repodata/&quot;&gt;repodata/&lt;/a&gt; &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 14:24 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; - &lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;rpcbind-0.2.0-38.el7_3.1.x86_64.rpm&quot;&gt;rpcbind-0.2.0-38.el7..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 59K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;socat-1.7.2.2-5.el7.x86_64.rpm&quot;&gt;socat-1.7.2.2-5.el7...&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;255K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;spax-1.5.2-13.el7.x86_64.rpm&quot;&gt;spax-1.5.2-13.el7.x8..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt;260K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;tcp_wrappers-7.6-77.el7.x86_64.rpm&quot;&gt;tcp_wrappers-7.6-77...&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 78K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;time-1.7-45.el7.x86_64.rpm&quot;&gt;time-1.7-45.el7.x86_..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 30K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;img src=&quot;/icons/unknown.gif&quot; alt=&quot;[ ]&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;tuned-profiles-origin-node-1.5.1-1.el7.x86_64.rpm&quot;&gt;tuned-profiles-origi..&amp;gt;&lt;/a&gt;&lt;/td&gt;&lt;td align=&quot;right&quot;&gt;2017-07-23 10:05 &lt;/td&gt;&lt;td align=&quot;right&quot;&gt; 11K&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;th colspan=&quot;5&quot;&gt;&lt;hr&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/table&gt; 到现在部署节点已经安装完毕；yum 源监听的是81端口，registry 监听的是4000端口 使用cobbler 安装node节点设置虚拟机从pxe启动在virtulbox要注意的一点是，要关闭virtulbox自带的dhcp；网络模式使用host-only 开始安装node节点使用上下键选择安装centos7， 按下table键设置ip地址，确认（enter）开始安装 安装openshift修改host文件12345[root@control01 byo]# vim /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.56.100 control01192.168.56.101 control02 设置inventory123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778[root@control01 byo]# vim /root/openshift-ansible/inventory/byo/hosts.simple[OSEv3:children]mastersnodesetcd[OSEv3:vars]ansible_ssh_user=rootopenshift_deployment_type=originopenshift_master_cluster_hostname=control02openshif_release=v1.5#### 注意这个all-in-one本来不该设置的，但是oepeshift内部使用域名访问openshift_master_cluster_method=nativeopenshift_master_cluster_public_hostname=192.168.56.101openshift_docker_options=&quot;-l warn --ipv6=false&quot;openshift_master_identity_providers=[&#123;&apos;name&apos;: &apos;htpasswd_auth&apos;, &apos;login&apos;: &apos;true&apos;, &apos;challenge&apos;: &apos;true&apos;, &apos;kind&apos;: &apos;HTPasswdPasswordIdentityProvider&apos;, &apos;filename&apos;: &apos;/etc/origin/master/htpasswd&apos;&#125;]openshift_additional_repos=[&#123;&apos;id&apos;: &apos;openshift-origin-local&apos;, &apos;name&apos;: &apos;OpenShift Origin&apos;, &apos;baseurl&apos;: &apos;http://192.168.56.100:81/openshift-package&apos;, &apos;enabled&apos;: 1, &apos;gpgcheck&apos;: 0&#125;]openshift_docker_additional_registries=192.168.56.100:4000openshift_docker_insecure_registries=0.0.0.0/0openshift_examples_modify_imagestreams=trueopenshift_cockpit_deployer_prefix=192.168.56.100:4000/openshift/openshift_cockpit=192.168.56.100:4000/openshift/cockpit/kubernetesopenshift_disable_check=memory_availability,disk_availability,package_version,package_availability,package_update,docker_image_availability,docker_storage_driver,docker_storage[masters]control02[etcd]control02[nodes]control02 openshift_schedulable=true openshift_node_labels=&quot;&#123;&apos;region&apos;: &apos;infra&apos;, &apos;zone&apos;: &apos;default&apos;&#125;&quot;### 测试ansible是否ok[root@control01 byo]# ansible -i hosts.simple all -m pingcontrol02 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;### 安装[root@control01 ~]# ansible-playbook ~/openshift-ansible/playbooks/byo/config.yml -i /root/openshift-ansible/inventory/byo/hosts.simplePLAY [Create initial host groups for localhost] ***************************************************************************************************************************************************************TASK [include_vars] *******************************************************************************************************************************************************************************************ok: [localhost]PLAY [Populate config host groups] ****************************************************************************************************************************************************************************TASK [Evaluate groups - g_etcd_hosts required] ****************************************************************************************************************************************************************skipping: [localhost]TASK [Evaluate groups - g_master_hosts or g_new_master_hosts required] ****************************************************************************************************************************************skipping: [localhost]TASK [openshift_excluder : Enable openshift excluder] *********************************************************************************************************************************************************changed: [control02]PLAY RECAP ****************************************************************************************************************************************************************************************************control02 : ok=598 changed=160 unreachable=0 failed=0localhost : ok=12 changed=0 unreachable=0 failed=0[root@control01 ~]#[root@control01 ~]#[root@control01 ~]#[root@control01 ~]# ssh control02Warning: Permanently added &apos;control02,192.168.56.101&apos; (ECDSA) to the list of known hosts.Last login: Sun Jul 30 08:12:54 2017 from 192.168.56.100[root@control02 ~]# kubectl get nodesNAME STATUS AGEcontrol02 Ready 8m[root@control02 ~]# kubectl get podsNAME READY STATUS RESTARTS AGEdocker-registry-1-2p331 1/1 Running 0 2mregistry-console-1-1rvtj 1/1 Running 0 1mrouter-1-fhf1x 1/1 Running 0 3m 安装结束，默认会安装registry、console、router","categories":[{"name":"openshift","slug":"openshift","permalink":"http://yoursite.com/child/categories/openshift/"}],"tags":[{"name":"openshift","slug":"openshift","permalink":"http://yoursite.com/child/tags/openshift/"},{"name":"pass","slug":"pass","permalink":"http://yoursite.com/child/tags/pass/"}]},{"title":"openshift v3.6 多节点安装","slug":"openshift/09-27-openshift-03","date":"2017-07-14T04:30:00.000Z","updated":"2017-10-09T06:42:56.000Z","comments":true,"path":"2017/07/14/openshift/09-27-openshift-03/","link":"","permalink":"http://yoursite.com/child/2017/07/14/openshift/09-27-openshift-03/","excerpt":"","text":"设置ssh 无密码登陆默认在 ~/.ssh目录生成两个文件： id_rsa ：私钥 id_rsa.pub ：公钥 1ssh-keygen -t rsa 导入本机1cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 导入要免密码登录的服务器,首先将公钥复制到服务器1scp ~/.ssh/id_rsa.pub xxx@host:/home/xxx/id_rsa.pub 在服务器上更改pub文件权限12chmod 700 ~/.sshchmod 600 ~/.ssh/authorized_keys 环境准备通过yum 安装 ansible1yum install ansible 修改/etc/ansible/hosts1234567openshift-lbopenshift-master-1openshift-master-2openshift-master-3openshift-node-1openshift-node-2openshift-node-3 测试ansible 是否可用12345678910111213141516171819202122232425[root@openshift-lb ansible]# ansible all -m pingopenshift-node-2 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;openshift-master-1 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;openshift-master-2 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;openshift-master-3 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;openshift-node-1 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;openshift-node-3 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125; 使用ansible 修改hosts文件1ansible all -m copy -a &quot;src=/etc/hosts dest=/etc/hosts&quot; bug123456789error: %preun(tuned-profiles-origin-node-3.6.0-1.0.c4dd4cf.x86_64) scriptlet failed, exit status 1Error in PREUN scriptlet in rpm package tuned-profiles-origin-node-3.6.0-1.0.c4dd4cf.x86_64 Verifying : tuned-profiles-origin-node-3.6.0-1.0.c4dd4cf.x86_64 1/1Failed: tuned-profiles-origin-node.x86_64 0:3.6.0-1.0.c4dd4cf解决方案rpm -e --noscripts --allmatches tuned-profiles-origin-node.x86_64 安装openshift下载openshift-ansible1git clone https://github.com/openshift/openshift-ansible.git 修改/etc/ansible/host文件12","categories":[{"name":"openshift","slug":"openshift","permalink":"http://yoursite.com/child/categories/openshift/"}],"tags":[{"name":"openshift","slug":"openshift","permalink":"http://yoursite.com/child/tags/openshift/"},{"name":"pass","slug":"pass","permalink":"http://yoursite.com/child/tags/pass/"}]},{"title":"制作 openshift iso","slug":"openshift/openshfit-iso","date":"2017-07-14T04:30:00.000Z","updated":"2017-09-22T01:42:39.000Z","comments":true,"path":"2017/07/14/openshift/openshfit-iso/","link":"","permalink":"http://yoursite.com/child/2017/07/14/openshift/openshfit-iso/","excerpt":"","text":"###制作iso过程文档 制作openshift isoFedora发行版制作的工具主要有revisor和pungi两种；revisor是一个图形化的工具，也可用命令行，但是经常会有bug，而且比较臃肿，感觉是给初级用户用的；pungi是Fedora官方制作（或叫spin）发行版的工具，命令行，总共也就4、5个Python文件；下面是使用 pungi来制作的iso 安装pungi以下都是环境都是基于centos7 123456789## 安装epel repoyum install -y epel-release## 安装elreporpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.orgrpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm## 安装 pungiyum install -y pungi hfsplus-tools kmod-hfs kmod-hfsplus docker 使用pungi build iso123456git clone http://gitlab.sh.99cloud.net/zhubingbing/openshift-iso.gitcd openshif-iso./build cobbler //先build cobbler tar包./build //等待10几分钟iso就build好了ls /root/kolla-iso/Ocata/x86_64/iso/CentOS-DVD-x86_64-Ocata.iso build.sh代码12345678910111213141516171819202122232425262728#!/bin/bash -ecase $1 in &apos;&apos;)pungi --name=CentOS --ver=Ocata -c ks.cfg --isfinal --nosource --nodebuginfo --force -G -C -Brsync -a isolinux.cfg Ocata/x86_64/os/isolinux/rsync -a os_ks.cfg Ocata/x86_64/os/images/ks.cfgmount Ocata/x86_64/os/images/efiboot.img /mntrsync -a grub.cfg /mnt/EFI/BOOT/rsync -a grub.cfg Ocata/x86_64/os/EFI/BOOT/umount /mntrm -f Ocata/x86_64/os/images/macboot.imgmkdir Ocata/x86_64/os/extrasrsync -a cobbler/init.sh Ocata/x86_64/os/extrasrsync -a cobbler/cobbler.tar Ocata/x86_64/os/extrasrsync -a openshift/registry.tar Ocata/x86_64/os/extrasrsync -a openshift/docker-registry.tar Ocata/x86_64/os/extrasrsync -a openshift/openshift-ansible.tar.gz Ocata/x86_64/os/extrasrsync -a openshift/openshift-package.tar Ocata/x86_64/os/extraspungi --name=CentOS --ver=Ocata -c ks.cfg --isfinal --nosource --nodebuginfo --force -I ;; &quot;cobbler&quot; )docker build -t cobbler cobbler/build/; docker save cobbler &gt; cobbler/cobbler.tar ;; &quot;clean&quot;)rm -rf centos centos-extras docker epel logs ourtree work ocata Ocata cobbler/cobbler.tar ;;esac 注意： 12345## 这些包自己预先准备好，因为过大就没放在github上面，路径是存在 openshif-iso/openshift/目录下面，如果没有这个目录自己建立[root@localhost openshift]# lscentos-openshift-origin.tar docker-registry.tar openshift-ansible.tar.gz openshift-package.tar registry.tar[root@localhost openshift]# du -h1.8G .","categories":[{"name":"openshift","slug":"openshift","permalink":"http://yoursite.com/child/categories/openshift/"}],"tags":[{"name":"openshift","slug":"openshift","permalink":"http://yoursite.com/child/tags/openshift/"}]},{"title":"Openshift-01","slug":"openshift/openshift介绍","date":"2017-07-14T04:21:00.000Z","updated":"2017-09-22T02:21:19.000Z","comments":true,"path":"2017/07/14/openshift/openshift介绍/","link":"","permalink":"http://yoursite.com/child/2017/07/14/openshift/openshift介绍/","excerpt":"","text":"openshift 架构介绍OpenShift是一个私有的PaaS（Platform-as-a-Service）解决方案，主要用来在容器中搭建、部署以及运行应用程序。它是基于Apache 2.0许可的开源软件， 并且发行了两个版本， 一个是社区版， 一个是企业版，由红帽主导。 OpenShift Origin从2014年7月开始，OpenShift就己经着力于研究一个非常出色的项目，该项目是将技术架构和与Docker、Kubernetes整合到一起（现在这是一件很常见的事）。 一年前启动这个项目对于OpenShift来说是一个大胆而且充满风险的的决策。确实如此，当时云平台的竞争处于白热化的巅峰时期，而此时OpenShift就决定冒着风险启动这样一个非常重要的重建项目，这个风险主要来自于他们需要停止新特性的开发并妥协旧版本之间的兼容性问题。但是现在，我们相信他们作了一个正确的决定。 到目前为止，社区版本联合了86名GitHub上（GitHub是红帽上10个最活跃项目之一）的开发志愿者。在12个月中己经进行了16次的迭代，并且刚刚发行了第一版。 尽管这个方案主要是由红帽在推动，但它非常依赖Kubernetes（来源于Google）。由此引发了到底由谁来支配和主导的问题，该问题依赖于两个公司之间的协作和版本基准，Google开发出新特性后会发生什么？他们会关注Kubernetes还是OpenShift？很显然，这个问题还没有答案。 但是，Google的技术支持部门己经确认，由两家公司同时主导只会给OpenShift带来更多益处。它将成为比Docker企业版更具竟争力的产品（机器、组建和群）。 OpenShift 技术堆栈 从技术堆栈的角度分析，作为一个容器云，OpenShift自底而上包含了以下几个层次：基础架构层、容器引擎层、容器编排层、PaaS服务层、界面及工具层 基础架构层基础架构层为OpenShift平台的运行提供了基础的运行环境。OpenShift支持运行在物理机、虚拟机、基础架构云（如OpenStack、Amazon Web Service、Microsoft Azure等）或混合云上。在操作系统层面，OpenShift支持多种不同的Linux操作系统，如企业级的Red Hat Enterprise Linux、社区的CentOS。 容器引擎层OpenShift目前以Docker作为平台的容器引擎。Docker是当前主流的容器引擎，已经在社区及许多企业的环境中进行了检验。事实证明Docker有能力为应用提供安全、稳定及高性能的运行环境。OpenShift运行的所有容器应用最终落到最底层的实现，其实就是一个个Docker容器实例。OpenShift对Docker整合是开放式的。OpenShift并没有修改Docker的任何代码，完全基于原生的Docker。熟悉Docker的用户对OpenShift能快速上手。同时，Docker现有的庞大的镜像资源都可以无缝地接入OpenShift平台。 容器编排层目前大家对容器编排的讨论已经成为容器相关话题中的一个热点。Kubernetes是Google在内部多年容器使用经验基础上的一次总结。Kubernetes设计的目的是满足在大规模集群环境下对容器的调度和部署的需求。Kubernetes是OpenShift的重要组件，OpenShift平台上的许多对象和概念都是衍生自Kubernetes，如Pod、Namespace、Replication Controller等。与对Docker的集成一样，OpenShift并没有尝试从代码上定制Kubernetes，OpenShift对Kubernetes的整合是叠加式的，在OpenShift集群上仍然可以通过Kubernetes的原生命令来操作Kubernetes的原生对象 Pass服务层Docker和Kubernetes为OpenShift提供了一个良好的基础，但是只有容器引擎和容器编排工具并不能大幅度提高生产效率，形成真正的生产力。正如Kubernetes在其主页上自我介绍所描述的那样，Kubernetes关注的核心是容器应用的编排和部署，它并不是一个完整的PaaS解决方案。容器平台最终的目的是向上层应用服务提供支持，加速应用开发、部署和运维的速度和效率。OpenShift在PaaS服务层默认提供了丰富的开发语言、开发框架、数据库及中间件的支持。用户可以在OpenShift这个平台上快速部署和获取一个数据库、分布式缓存或者业务规则引擎的服务。除了Docker Hub上的社区镜像外，OpenShift还有一个重要的服务提供方：Red Hat。Red Hat旗下的JBoss中间件系列几乎全线的产品都已经容器化。JBoss中间件包含了开发框架、开发工具、应用服务器、消息中间件、SOA套件、业务流程平台（BPM）、单点登录、应用监控、应用性能管理（APM）、分布式缓存及数据虚拟化等产品。这些中间件可以直接通过OpenShift容器云对用户提供服务。通过OpenShift，可以快速搭建一个Database as a Service，即DBaaS，一个BPMaaS，或者Redis-aaS等。 界面云平台一个很重要的特点是强调用户的自助服务，从而降低运维成本，提高服务效率。界面和工具是容器云平台上的最后一公里接入，好的界面和工具集合能帮助用户更高效地完成相关的任务。OpenShift提供了自动化流程Source to Image，即S2I，帮助用户容器化用各种编程语言开发的应用源代码。用户可以直接使用S2I或者把现有的流程与S2I整合，从而实现开发流程的持续集成和持续交付。提升开发、测试和部署的自动化程度，最终提高开发、测试及部署的效率，缩短上市时间。OpenShift提供了多种用户的接入渠道：Web控制台、命令行、IDE集成及RESTful编程接口。这些都是一个完善的企业级平台必不可少的组件。针对容器应用的运维及集群的运维，OpenShift提供了性能度量采集、日志聚合模块及运维管理套件，帮助运维用户完成日常的应用及集群运维任务","categories":[{"name":"openshift","slug":"openshift","permalink":"http://yoursite.com/child/categories/openshift/"}],"tags":[{"name":"openshift","slug":"openshift","permalink":"http://yoursite.com/child/tags/openshift/"},{"name":"pass","slug":"pass","permalink":"http://yoursite.com/child/tags/pass/"}]},{"title":"kolla之reconfigure","slug":"openstack/kolla-reconfigure","date":"2017-07-09T03:30:00.000Z","updated":"2017-09-22T02:21:06.000Z","comments":true,"path":"2017/07/09/openstack/kolla-reconfigure/","link":"","permalink":"http://yoursite.com/child/2017/07/09/openstack/kolla-reconfigure/","excerpt":"","text":"今天好好理理kolla reconfigure如何玩的 kolla的配置管理主要是管理openstack service config文件；主要实现是 1kolla-ansible reconfigure reconfigure 使用下面以1个例子来演示下 修改nova.conf,重启相关服务 在部署节点 新建／etc/kolla/config/nova/nova.conf 123456### 修改rpc_response超时时间vim ／etc/kolla/config/nova/nova.confrpc_response_timeout = 350## 执行reconfigurekolla-ansible reconfigure reconfigure 代码流程kolla-ansible 的核心代码在ansible实现的先介绍下ansible role是什么？如何使用先看看下面nova role的代码目录 12345678910111213141516[root@control01 ansible]# tree -d.├── action_plugins├── group_vars├── inventory├── library└── roles ├── nova │ ├── defaults │ ├── handlers │ ├── meta │ ├── tasks │ └── templates . . . ansible role是什么？Ansible Role 是一种分类 &amp; 重用的概念，透过将 vars, tasks, files, templates, handler … 等等根据不同的目的(例如：nova、glance、cinder)，规划后至于独立目录中，后续便可以利用 include 的概念來使用。 若同样是 include 的概念，那 role 跟 include 之间不一样的地方又是在哪里呢? 答案是：role 的 include 机制是自动的! 我們只要事前將 role 的 vars / tasks / files / handler …. 等等事先定义好按照特定的结构(下面會提到)放好，Ansible 就會很聰明的幫我們 include 完成，不需要再自己一个一个指定 include。 透过这样的方式，管理者可以透过設定 role 的方式將所需要安裝設定的功能分门别类，拆成细项來管理并编写相对应的 script，让原本可能很庞大的设定工作可以细分成多个不同的部分來分別设定，不仅仅可以让自己重复利用特定的设定，也可以共享給其他人一同使用。 要设计一個 role，必須先知道怎麼將 files / templates / tasks / handlers / vars …. 等等拆开设定 看看下面kolla-ansible下nova role的代码目录 12345678910111213141516[root@control01 ansible]# tree -d.├── action_plugins├── group_vars├── inventory├── library└── roles ├── nova │ ├── defaults │ ├── handlers │ ├── meta │ ├── tasks │ └── templates . . . 以上就是一个基本完整的一个role结构，当然还有file、vars；我们这里没有使用这个2个；如果没有的部分可以不用。 ansible会针对role（x）进行以下处理： 如果role/x／tasks／main.yml存在，则会自动加到playbook中的task list中 如果role/x／handlers／main.yml存在，则会自动加到playbook中的handler list中 如果role/x/vars/mani.yml存在，则会自动加入到playbook中的variables list中 如果role/x/meta/main.yml存在, 任何与指定的role想依赖的其他的role设置都会被自动加入 roles/x/templates/ 目录中的 template tasks，在 playbook 中使用时不需要指定绝对(absolutely) or 相对(relatively)路径 在 roles/x/files/ 目录中的 copy tasks 或是 script tasks，在 playbook 中使用时不需要指定绝对(absolutely) or 相对(relatively)路径 定义在 roles/x/defaults/main.yml 中的变量將会是使用该role时所取得的预设变量 再返回来看来 kolla-ansible reconfigure 具体代码就能理解了 12[root@control01 ansible]# kolla-ansible reconfigureReconfigure OpenStack service : ansible-playbook -i /usr/share/kolla-ansible/ansible/inventory/all-in-one -e @/etc/kolla/globals.yml -e @/etc/kolla/passwords.yml -e CONFIG_DIR=/etc/kolla -e action=reconfigure -e serial=0 /usr/share/kolla-ansible/ansible/site.yml 1234## glance tasks main.yaml[root@control01 tasks]# cat /root/kolla-ansible/ansible/roles/glance/tasks/main.yml---- include: &quot;&#123;&#123; action &#125;&#125;.yml&quot; 从上面命令就可以看出，tasks是根据action去找相应的操作。那我们去跳到reconfigure.yml 123[root@control01 tasks]# cat reconfigure.yml---- include: deploy.yml 12345678910111213141516171819202122232425262728293031[root@control01 tasks]# cat deploy.yml---- include: ceph.yml when: - (enable_ceph | bool) and (glance_backend_ceph | bool) - inventory_hostname in groups[&apos;ceph-mon&apos;] or inventory_hostname in groups[&apos;glance-api&apos;] or inventory_hostname in groups[&apos;glance-registry&apos;]- include: external_ceph.yml when: - (enable_ceph | bool == False) and (glance_backend_ceph | bool) - inventory_hostname in groups[&apos;glance-api&apos;] or inventory_hostname in groups[&apos;glance-registry&apos;]- include: register.yml when: inventory_hostname in groups[&apos;glance-api&apos;]- include: config.yml when: inventory_hostname in groups[&apos;glance-api&apos;] or inventory_hostname in groups[&apos;glance-registry&apos;]- include: bootstrap.yml when: inventory_hostname in groups[&apos;glance-api&apos;]- name: Flush handlers meta: flush_handlers- include: check.yml when: inventory_hostname in groups[&apos;glance-api&apos;] or inventory_hostname in groups[&apos;glance-registry&apos;] 整体的一个脉络流程就是这样；下面我们一个一个去细看他们的实现。 这里我们主要是看config.yml和flush_handlers这个2个的实现 config.ymlconfig.yml 是用来生成openstack sevice config文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182[root@control01 tasks]# cat config.yml---### 生成 node_config_directory（默认是/etc/kolla， 你可以在all.yml中设置）sevice目录- name: Ensuring config directories exist file: path: &quot;&#123;&#123; node_config_directory &#125;&#125;/&#123;&#123; item.key &#125;&#125;&quot; state: &quot;directory&quot; recurse: yes when: - inventory_hostname in groups[item.value.group] - item.value.enabled | bool with_dict: &quot;&#123;&#123; glance_services &#125;&#125;&quot;### copy config.json file 到node_config_directory/service 目录下- name: Copying over config.json files for services template: src: &quot;&#123;&#123; item.key &#125;&#125;.json.j2&quot; dest: &quot;&#123;&#123; node_config_directory &#125;&#125;/&#123;&#123; item.key &#125;&#125;/config.json&quot; register: glance_config_jsons when: - item.value.enabled | bool - inventory_hostname in groups[item.value.group] with_dict: &quot;&#123;&#123; glance_services &#125;&#125;&quot; notify: - Restart glance-api container - Restart glance-registry container- name: Copying over glance-*.conf merge_configs: vars: service_name: &quot;&#123;&#123; item.key &#125;&#125;&quot; sources: - &quot;&#123;&#123; role_path &#125;&#125;/templates/&#123;&#123; item.key &#125;&#125;.conf.j2&quot; - &quot;&#123;&#123; node_custom_config &#125;&#125;/global.conf&quot; - &quot;&#123;&#123; node_custom_config &#125;&#125;/database.conf&quot; - &quot;&#123;&#123; node_custom_config &#125;&#125;/messaging.conf&quot; - &quot;&#123;&#123; node_custom_config &#125;&#125;/glance.conf&quot; - &quot;&#123;&#123; node_custom_config &#125;&#125;/glance/&#123;&#123; item.key &#125;&#125;.conf&quot; - &quot;&#123;&#123; node_custom_config &#125;&#125;/glance/&#123;&#123; inventory_hostname &#125;&#125;/&#123;&#123; item.key &#125;&#125;.conf&quot; dest: &quot;&#123;&#123; node_config_directory &#125;&#125;/&#123;&#123; item.key &#125;&#125;/&#123;&#123; item.key &#125;&#125;.conf&quot; register: glance_confs when: - item.value.enabled | bool - inventory_hostname in groups[item.value.group] with_dict: &quot;&#123;&#123; glance_services &#125;&#125;&quot; notify: - Restart glance-api container - Restart glance-registry container- name: Check if policies shall be overwritten local_action: stat path=&quot;&#123;&#123; node_custom_config &#125;&#125;/glance/policy.json&quot; register: glance_policy- name: Copying over existing policy.json template: src: &quot;&#123;&#123; node_custom_config &#125;&#125;/glance/policy.json&quot; dest: &quot;&#123;&#123; node_config_directory &#125;&#125;/&#123;&#123; item.key &#125;&#125;/policy.json&quot; register: glance_policy_jsons when: - glance_policy.stat.exists - inventory_hostname in groups[item.value.group] with_dict: &quot;&#123;&#123; glance_services &#125;&#125;&quot; notify: - Restart glance-api container - Restart glance-registry container- name: Check glance containers kolla_docker: action: &quot;compare_container&quot; common_options: &quot;&#123;&#123; docker_common_options &#125;&#125;&quot; name: &quot;&#123;&#123; item.value.container_name &#125;&#125;&quot; image: &quot;&#123;&#123; item.value.image &#125;&#125;&quot; volumes: &quot;&#123;&#123; item.value.volumes &#125;&#125;&quot; register: check_glance_containers when: - action != &quot;config&quot; - inventory_hostname in groups[item.value.group] - item.value.enabled | bool with_dict: &quot;&#123;&#123; glance_services &#125;&#125;&quot; notify: - Restart glance-api container - Restart glance-registry container 这里最核心的实现就是merge_configs。action plugin中在merge_configs.py作用是导入template模板，并且run。代码在/usr/share/kolla-ansible/ansible/action_plugins/merge_configs.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138import collectionsimport inspectimport osfrom ansible.plugins import actionfrom six import StringIOfrom oslo_config import iniparserclass OverrideConfigParser(iniparser.BaseParser): def __init__(self): self._cur_sections = collections.OrderedDict() self._sections = collections.OrderedDict() self._cur_section = None def assignment(self, key, value): cur_value = self._cur_section.get(key) if len(value) == 1 and value[0] == &apos;&apos;: value = [] if not cur_value: self._cur_section[key] = [value] else: self._cur_section[key].append(value) def parse(self, lineiter): self._cur_sections = collections.OrderedDict() super(OverrideConfigParser, self).parse(lineiter) # merge _cur_sections into _sections for section, values in self._cur_sections.items(): if section not in self._sections: self._sections[section] = collections.OrderedDict() for key, value in values.items(): self._sections[section][key] = value def new_section(self, section): cur_section = self._cur_sections.get(section) if not cur_section: cur_section = collections.OrderedDict() self._cur_sections[section] = cur_section self._cur_section = cur_section return cur_section def write(self, fp): def write_key_value(key, values): for v in values: if not v: fp.write(&apos;&#123;&#125; =\\n&apos;.format(key)) for index, value in enumerate(v): if index == 0: fp.write(&apos;&#123;&#125; = &#123;&#125;\\n&apos;.format(key, value)) else: fp.write(&apos;&#123;&#125; &#123;&#125;\\n&apos;.format(len(key)*&apos; &apos;, value)) def write_section(section): for key, values in section.items(): write_key_value(key, values) for section in self._sections: fp.write(&apos;[&#123;&#125;]\\n&apos;.format(section)) write_section(self._sections[section]) fp.write(&apos;\\n&apos;)class ActionModule(action.ActionBase): TRANSFERS_FILES = True def read_config(self, source, config): # Only use config if present if os.access(source, os.R_OK): with open(source, &apos;r&apos;) as f: template_data = f.read() result = self._templar.template(template_data) fakefile = StringIO(result) config.parse(fakefile) fakefile.close() def run(self, tmp=None, task_vars=None): if task_vars is None: task_vars = dict() result = super(ActionModule, self).run(tmp, task_vars) # NOTE(jeffrey4l): Ansible 2.1 add a remote_user param to the # _make_tmp_path function. inspect the number of the args here. In # this way, ansible 2.0 and ansible 2.1 are both supported make_tmp_path_args = inspect.getargspec(self._make_tmp_path)[0] if not tmp and len(make_tmp_path_args) == 1: tmp = self._make_tmp_path() if not tmp and len(make_tmp_path_args) == 2: remote_user = (task_vars.get(&apos;ansible_user&apos;) or self._play_context.remote_user) tmp = self._make_tmp_path(remote_user) sources = self._task.args.get(&apos;sources&apos;, None) extra_vars = self._task.args.get(&apos;vars&apos;, list()) if not isinstance(sources, list): sources = [sources] temp_vars = task_vars.copy() temp_vars.update(extra_vars) config = OverrideConfigParser() old_vars = self._templar._available_variables self._templar.set_available_variables(temp_vars) for source in sources: self.read_config(source, config) self._templar.set_available_variables(old_vars) # Dump configparser to string via an emulated file fakefile = StringIO() config.write(fakefile) remote_path = self._connection._shell.join_path(tmp, &apos;src&apos;) xfered = self._transfer_data(remote_path, fakefile.getvalue()) fakefile.close() new_module_args = self._task.args.copy() new_module_args.pop(&apos;vars&apos;, None) new_module_args.pop(&apos;sources&apos;, None) new_module_args.update( dict( src=xfered ) ) result.update(self._execute_module(module_name=&apos;copy&apos;, module_args=new_module_args, task_vars=task_vars, tmp=tmp)) return result","categories":[{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/child/categories/openstack/"}],"tags":[{"name":"kolla","slug":"kolla","permalink":"http://yoursite.com/child/tags/kolla/"},{"name":"openstack","slug":"openstack","permalink":"http://yoursite.com/child/tags/openstack/"}]}]}